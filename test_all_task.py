"""
RunPod Pod ì„œë²„ API ì „ì²´ ê¸°ëŠ¥ í†µí•© í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ (ë‹¤ì¤‘ ëª¨ë¸ ì§€ì›)

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” RunPod Podì—ì„œ ì‹¤í–‰ ì¤‘ì¸ FastAPI ì„œë²„ë¥¼ ëŒ€ìƒìœ¼ë¡œ í…ŒìŠ¤íŠ¸ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
ì„±ëŠ¥ ì¸¡ì •, ì •í™•ë„ ì¸¡ì •, ì—¬ëŸ¬ ëª¨ë¸ ë¹„êµë¥¼ ì§€ì›í•©ë‹ˆë‹¤.

ì‚¬ìš© ë°©ë²•:
    ========================================
    1. ëª¨ë¸
    ========================================
    
    # jinsoo1218/runpod_vllm:latest
    # runpod_env
    
    llm:
    Qwen/Qwen2.5-7B-Instruct
    meta-llama/Llama-3.1-8B-Instruct
    google/gemma-2-9b-it

    Embedding:

    jhgan/ko-sbert-multitask
    dragonkue/BGE-m3-ko
    upskyy/bge-m3-korean
    
    ========================================
    2. ì„±ëŠ¥ ì¸¡ì • ëª¨ë“œ (ë²¤ì¹˜ë§ˆí¬)
    ========================================
    
    python test_openai_all.py --benchmark
    python test_openai_all.py --benchmark --iterations 10
    
    ========================================
    4. ê²°ê³¼ ì €ì¥
    ========================================
    
    ì´ˆê¸° ì…‹íŒ…
    qwen, kakao-app, sbert, 
    
    1. llm ëª¨ë¸ ë¹„êµ
    python test_openai_all.py --benchmark --iterations 3 --save-results qwen.json
    python test_openai_all.py --benchmark --iterations 3 --save-results llama.json
    python test_openai_all.py --benchmark --iterations 3 --save-results gemma.json
    
    3. llm ëª¨ë¸ ê³ ì •, sentiment ëª¨ë¸ ë¹„êµ
    python test_openai_all.py --benchmark --iterations 3 --save-results qwen_Kakao-app.json
    python test_openai_all.py --benchmark --iterations 3 --save-results qwen_klue-roberta.json
    python test_openai_all.py --benchmark --iterations 3 --save-results qwen_kcelectra.json
    
    3. llm ëª¨ë¸ ê³ ì •, sentiment ëª¨ë¸ ê³ ì •, embedding ëª¨ë¸ ë¹„êµ
    python test_openai_all.py --benchmark --iterations 3 --save-results qwen_Kakao-app_sbert.json
    python test_openai_all.py --benchmark --iterations 3 --save-results qwen_Kakao-app_bge-m3-drag.json
    python test_openai_all.py --benchmark --iterations 3 --save-results qwen_Kakao-app_bge-m3-upsky.json
    
    ========================================
    ì£¼ìš” ì˜µì…˜
    ========================================
    
    --benchmark: ì„±ëŠ¥ ì¸¡ì • ëª¨ë“œ í™œì„±í™” (ì²˜ë¦¬ ì‹œê°„, TTFT, TPS ë“±)
    --compare-models: ì—¬ëŸ¬ ëª¨ë¸ ë¹„êµ ëª¨ë“œ
    --models: ë¹„êµí•  ëª¨ë¸ëª… ë¦¬ìŠ¤íŠ¸ (--compare-modelsì™€ í•¨ê»˜ ì‚¬ìš©)
    --provider: LLM ì œê³µì (openai, local, runpod)
    --iterations: ì„±ëŠ¥ ì¸¡ì • ë°˜ë³µ íšŸìˆ˜ (ê¸°ë³¸ê°’: 5)
    --save-results: ê²°ê³¼ë¥¼ ì €ì¥í•  JSON íŒŒì¼ ê²½ë¡œ
    --generate-report: ëª¨ë¸ ë¹„êµ ë¦¬í¬íŠ¸ ìƒì„±
    
    ========================================
    ì¸¡ì • ì§€í‘œ
    ========================================
    
    ì„±ëŠ¥ ì§€í‘œ:
    - ì²˜ë¦¬ ì‹œê°„ (í‰ê· , P95, P99)
    - TTFT (Time To First Token)
    - TPS (Tokens Per Second)
    - ì²˜ë¦¬ëŸ‰ (req/s)
    
    ì •í™•ë„ ì§€í‘œ:
    - BLEU Score (ìš”ì•½)
    - ROUGE Score (ìš”ì•½)
    - Precision@K (ê°•ì  ì¶”ì¶œ)
    - MAE (ê°ì„± ë¶„ì„)
"""

import os
import sys
import json
import time
import requests
import subprocess
import tempfile
import argparse
import statistics
import sqlite3
import psutil
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

try:
    from src.metrics_collector import MetricsCollector
    METRICS_COLLECTOR_AVAILABLE = True
except ImportError:
    METRICS_COLLECTOR_AVAILABLE = False

try:
    from scripts.gpu_monitor import GPUMonitor
    GPU_MONITOR_AVAILABLE = True
except ImportError:
    GPU_MONITOR_AVAILABLE = False

# ìƒ‰ìƒ ì¶œë ¥ì„ ìœ„í•œ ANSI ì½”ë“œ
class Colors:
    GREEN = '\033[92m'
    RED = '\033[91m'
    YELLOW = '\033[93m'
    BLUE = '\033[94m'
    RESET = '\033[0m'
    BOLD = '\033[1m'

def print_success(msg: str):
    print(f"{Colors.GREEN}âœ“ {msg}{Colors.RESET}")

def print_error(msg: str):
    print(f"{Colors.RED}âœ— {msg}{Colors.RESET}")

def print_info(msg: str):
    print(f"{Colors.BLUE}â„¹ {msg}{Colors.RESET}")

def print_warning(msg: str):
    print(f"{Colors.YELLOW}âš  {msg}{Colors.RESET}")

try:
    from scripts.evaluate_sentiment_analysis import SentimentAnalysisEvaluator
    from scripts.evaluate_summary import SummaryEvaluator
    from scripts.evaluate_strength_extraction import StrengthExtractionEvaluator
    from scripts.evaluate_vector_search import PrecisionAtKEvaluator
    EVALUATION_AVAILABLE = True
except ImportError:
    EVALUATION_AVAILABLE = False
    print_warning("í‰ê°€ ìŠ¤í¬ë¦½íŠ¸ë¥¼ importí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì •í™•ë„ ì¸¡ì •ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")

def print_header(msg: str):
    print(f"\n{Colors.BOLD}{Colors.BLUE}{'='*60}{Colors.RESET}")
    print(f"{Colors.BOLD}{Colors.BLUE}{msg}{Colors.RESET}")
    print(f"{Colors.BOLD}{Colors.BLUE}{'='*60}{Colors.RESET}\n")

# jinsoo1218/runpod_vllm:latest
# runpod_env
# í…ŒìŠ¤íŠ¸ ì„¤ì •
# RunPod Pod ì„œë²„ URL (í™˜ê²½ ë³€ìˆ˜ë¡œ ì˜¤ë²„ë¼ì´ë“œ ê°€ëŠ¥)
#BASE_URL = "http://213.192.2.74:40162"  # RunPod Pod IP:í¬íŠ¸ë¡œ ë³€ê²½ (ì˜ˆ: http://213.192.2.68:40183)
BASE_URL = "http://localhost:8001"
API_PREFIX = "/api/v1"
METRICS_DB_PATH = "metrics.db"

# ìƒ˜í”Œ ë°ì´í„° (ë°ì´í„° ìƒì„± í›„ ì—…ë°ì´íŠ¸ë¨)
SAMPLE_RESTAURANT_ID = 1
SAMPLE_REVIEWS = []

# í…ŒìŠ¤íŠ¸ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ìš© ì „ì—­ ë”•ì…”ë„ˆë¦¬ (JSON ì €ì¥ìš©)
test_metrics: Dict[str, Any] = {}


def safe_json_response(response, error_msg="ì‘ë‹µ ì²˜ë¦¬ ì‹¤íŒ¨", allow_404=False):
    """ì•ˆì „í•˜ê²Œ JSON ì‘ë‹µ íŒŒì‹± (runpod_pod_all_test.py ì°¸ê³ )"""
    try:
        # 404ëŠ” ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ìƒ ì •ìƒ ì‘ë‹µì¼ ìˆ˜ ìˆìŒ (ë°ì´í„° ì—†ìŒ ë“±)
        if response.status_code == 404 and allow_404:
            try:
                error_detail = response.json()
                if "detail" in error_detail:
                    print(f"   â„¹ï¸ ì •ë³´: {error_detail['detail']}")
                    return error_detail  # 404 ì‘ë‹µë„ ë°˜í™˜
            except:
                pass
        
        response.raise_for_status()  # HTTP ì˜¤ë¥˜ í™•ì¸
        if not response.text:
            print(f"   âš ï¸ ë¹ˆ ì‘ë‹µ ë°˜í™˜")
            return None
        return response.json()
    except requests.exceptions.HTTPError as e:
        # 404ëŠ” ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ìƒ ì •ìƒì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë³„ë„ ì²˜ë¦¬
        if response.status_code == 404:
            try:
                error_detail = response.json()
                if "detail" in error_detail:
                    print(f"   â„¹ï¸ ì •ë³´: {error_detail['detail']}")
                    if allow_404:
                        return error_detail
                    else:
                        print(f"   âš ï¸ ë¦¬ì†ŒìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (ì •ìƒì¼ ìˆ˜ ìˆìŒ)")
                        return None
            except:
                pass
        
        print(f"   âš ï¸ HTTP ì˜¤ë¥˜: {e}")
        print(f"   ìƒíƒœ ì½”ë“œ: {response.status_code}")
        
        # ìƒì„¸ ì˜¤ë¥˜ ë©”ì‹œì§€ ì¶”ì¶œ ì‹œë„
        try:
            error_detail = response.json()
            if "detail" in error_detail:
                print(f"   ì˜¤ë¥˜ ìƒì„¸: {error_detail['detail']}")
            else:
                print(f"   ì‘ë‹µ ë‚´ìš©: {json.dumps(error_detail, ensure_ascii=False, indent=2)[:500]}")
        except:
            print(f"   ì‘ë‹µ ë‚´ìš© (í…ìŠ¤íŠ¸): {response.text[:500]}")
        
        return None
    except json.JSONDecodeError as e:
        print(f"   âš ï¸ JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
        print(f"   ì‘ë‹µ ë‚´ìš©: {response.text[:500]}")
        print(f"   ìƒíƒœ ì½”ë“œ: {response.status_code}")
        return None
    except Exception as e:
        print(f"   âš ï¸ {error_msg}: {e}")
        return None


def check_server_health():
    """ì„œë²„ í—¬ìŠ¤ ì²´í¬ (RunPod Pod ì„œë²„ìš©)"""
    try:
        start_time = time.time()
        response = requests.get(f"{BASE_URL}/health", timeout=10)
        elapsed_time = time.time() - start_time
        result = safe_json_response(response, "í—¬ìŠ¤ ì²´í¬ ì‹¤íŒ¨")
        if result:
            print_success(f"ì„œë²„ ì—°ê²° ì„±ê³µ: {result}")
            print_info(f"   â±ï¸ ì‘ë‹µ ì‹œê°„: {elapsed_time:.2f}ì´ˆ")
            print_info(f"   ì„œë²„ URL: {BASE_URL}")
            return True
        else:
            print_error("í—¬ìŠ¤ ì²´í¬ ì‹¤íŒ¨: ì‘ë‹µì„ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return False
    except Exception as e:
        print_error(f"í—¬ìŠ¤ ì²´í¬ ì‹¤íŒ¨: {e}")
        print_info(f"ì„œë²„ URL: {BASE_URL}")
        print_info("ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš” (RunPod Podì—ì„œ FastAPI ì„œë²„ í™•ì¸)")
        return False


def generate_test_data(
    generate_from_kr3: bool = False,
    kr3_sample: Optional[int] = None,
    kr3_restaurants: Optional[int] = None,
):
    """
    í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ë˜ëŠ” ìƒì„±
    
    Args:
        generate_from_kr3: kr3.tsvì—ì„œ ë°ì´í„° ìƒì„± ì—¬ë¶€
        kr3_sample: kr3.tsvì—ì„œ ìƒ˜í”Œë§í•  ë¦¬ë·° ìˆ˜
        kr3_restaurants: ìƒì„±í•  ë ˆìŠ¤í† ë‘ ìˆ˜
    """
    # kr3.tsvì—ì„œ ë°ì´í„° ìƒì„± ëª¨ë“œ
    if generate_from_kr3:
        return generate_test_data_from_kr3(kr3_sample, kr3_restaurants)
    
    # ê¸°ë³¸: test_data_sample.json íŒŒì¼ì—ì„œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
    print_header("í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ")
    
    # test_data_sample.json íŒŒì¼ ê²½ë¡œ
    test_data_path = project_root / "data" / "test_data_sample.json"
    
    if not test_data_path.exists():
        print_warning(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {test_data_path}")
        print_info("ëŒ€ì²´ ë°©ë²•: --generate-from-kr3 ì˜µì…˜ìœ¼ë¡œ kr3.tsvì—ì„œ ë°ì´í„°ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        return None
    
    try:
        # JSON íŒŒì¼ ì½ê¸°
        print_info(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° íŒŒì¼ ë¡œë“œ ì¤‘: {test_data_path}")
        with open(test_data_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        restaurants_count = len(data.get('restaurants', []))
        print_success(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {restaurants_count}ê°œ ë ˆìŠ¤í† ë‘")
        
        # ì´ ë¦¬ë·° ìˆ˜ ê³„ì‚°
        total_reviews = sum(
            len(restaurant.get('reviews', []))
            for restaurant in data.get('restaurants', [])
        )
        print_info(f"  - ì´ ë¦¬ë·° ìˆ˜: {total_reviews}ê°œ")
        
        # ì„ì‹œ íŒŒì¼ ê²½ë¡œëŠ” None ë°˜í™˜ (ë” ì´ìƒ í•„ìš” ì—†ìŒ)
        return data, None
        
    except json.JSONDecodeError as e:
        print_error(f"JSON íŒŒì¼ íŒŒì‹± ì˜¤ë¥˜: {str(e)}")
        return None
    except Exception as e:
        print_error(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None


def generate_test_data_from_kr3(
    sample: Optional[int] = None,
    restaurants: Optional[int] = None,
):
    """kr3.tsv íŒŒì¼ì—ì„œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±"""
    print_header("kr3.tsvì—ì„œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±")
    
    # kr3.tsv íŒŒì¼ í™•ì¸
    kr3_path = project_root / "data" / "kr3.tsv"
    if not kr3_path.exists():
        print_error(f"kr3.tsv íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {kr3_path}")
        return None
    
    # ì„ì‹œ JSON íŒŒì¼ ìƒì„±
    temp_file = tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False)
    temp_file.close()
    temp_json_path = temp_file.name
    
    try:
        # convert_kr3_tsv.py ì‹¤í–‰
        print_info("kr3.tsvì—ì„œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± ì¤‘...")
        cmd = [
            sys.executable,
            str(project_root / "scripts" / "convert_kr3_tsv.py"),
            "--input", str(kr3_path),
            "--output", temp_json_path,
        ]
        
        # ìƒ˜í”Œë§ ì˜µì…˜ ì¶”ê°€
        if sample:
            cmd.extend(["--sample", str(sample)])
        
        # ë ˆìŠ¤í† ë‘ ìˆ˜ ì˜µì…˜ ì¶”ê°€
        if restaurants:
            cmd.extend(["--restaurants", str(restaurants)])
        
        print_info(f"ì‹¤í–‰ ëª…ë ¹: {' '.join(cmd)}")
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)
        
        if result.returncode != 0:
            print_error(f"ë°ì´í„° ìƒì„± ì‹¤íŒ¨: {result.stderr}")
            if os.path.exists(temp_json_path):
                os.unlink(temp_json_path)
            return None
        
        # ìƒì„±ëœ JSON íŒŒì¼ ì½ê¸°
        with open(temp_json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        restaurants_count = len(data.get('restaurants', []))
        print_success(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± ì™„ë£Œ: {restaurants_count}ê°œ ë ˆìŠ¤í† ë‘")
        
        # ì´ ë¦¬ë·° ìˆ˜ ê³„ì‚°
        total_reviews = sum(
            len(restaurant.get('reviews', []))
            for restaurant in data.get('restaurants', [])
        )
        print_info(f"  - ì´ ë¦¬ë·° ìˆ˜: {total_reviews}ê°œ")
        
        return data, temp_json_path
        
    except subprocess.TimeoutExpired:
        print_error("ë°ì´í„° ìƒì„± ì‹œê°„ ì´ˆê³¼ (300ì´ˆ)")
        if os.path.exists(temp_json_path):
            os.unlink(temp_json_path)
        return None
    except Exception as e:
        print_error(f"ë°ì´í„° ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
        if os.path.exists(temp_json_path):
            os.unlink(temp_json_path)
        return None


def upload_data_to_qdrant(data: Dict[str, Any]):
    """ìƒì„±ëœ ë°ì´í„°ë¥¼ Qdrantì— upload"""
    print_header("Qdrantì— ë°ì´í„° Upload")
    
    if not data or "restaurants" not in data:
        print_warning("Uploadí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return False
    
    url = f"{BASE_URL}{API_PREFIX}/vector/upload"
    
    # ëª¨ë“  ë¦¬ë·°ì™€ ë ˆìŠ¤í† ë‘ ì •ë³´ë¥¼ ìˆ˜ì§‘
    all_reviews = []
    all_restaurants = []
    
    for restaurant_data in data["restaurants"]:
        # ë ˆìŠ¤í† ë‘ ì •ë³´ ì¶”ê°€
        restaurant_id = restaurant_data.get("restaurant_id")
        restaurant_info = {
            "id": int(restaurant_id) if isinstance(restaurant_id, (int, str)) and str(restaurant_id).isdigit() else restaurant_id,
            "name": restaurant_data.get("restaurant_name", f"Test Restaurant {restaurant_id}"),
            "full_address": None,
            "location": None,
            "created_at": None,
            "deleted_at": None
        }
        all_restaurants.append(restaurant_info)
        
        # ë¦¬ë·° ì •ë³´ ì¶”ê°€ (restaurant_idë¥¼ intë¡œ ë³€í™˜)
        reviews = restaurant_data.get("reviews", [])
        for review in reviews:
            # restaurant_idë¥¼ intë¡œ ë³€í™˜ (ReviewModelì´ intë¥¼ ê¸°ëŒ€)
            review_copy = review.copy()
            if "restaurant_id" in review_copy:
                review_copy["restaurant_id"] = int(review_copy["restaurant_id"]) if isinstance(review_copy["restaurant_id"], str) and str(review_copy["restaurant_id"]).isdigit() else review_copy["restaurant_id"]
            all_reviews.append(review_copy)
    
    try:
        payload = {
            "reviews": all_reviews,
            "restaurants": all_restaurants
        }
        
        start_time = time.time()
        response = requests.post(url, json=payload, timeout=300)  # ëŒ€ìš©ëŸ‰ ë°ì´í„°ë¥¼ ìœ„í•´ íƒ€ì„ì•„ì›ƒ ì¦ê°€
        elapsed_time = time.time() - start_time
        result = safe_json_response(response, "ì—…ë¡œë“œ ì‹¤íŒ¨")
        
        if result:
            points_count = result.get("points_count", 0)
            print_success(f"ì´ {points_count}ê°œ í¬ì¸íŠ¸ê°€ Qdrantì— uploadë˜ì—ˆìŠµë‹ˆë‹¤.")
            print_info(f"  - ë¦¬ë·°: {len(all_reviews)}ê°œ")
            print_info(f"  - ë ˆìŠ¤í† ë‘: {len(all_restaurants)}ê°œ")
            print_info(f"  â±ï¸ ì‘ë‹µ ì‹œê°„: {elapsed_time:.2f}ì´ˆ")
            return True
        else:
            print_warning("Upload ì‹¤íŒ¨")
            print_info("ğŸ’¡ í•´ê²° ë°©ë²•:")
            print_info("   1. RUNPOD í™˜ê²½ ë³€ìˆ˜ì— QDRANT_URL=:memory: ì„¤ì • (ì¸ë©”ëª¨ë¦¬ ì‚¬ìš©)")
            print_info("   2. ë˜ëŠ” ì™¸ë¶€ Qdrant ì„œë²„ URL ì„¤ì •")
            print_info("   3. ì„œë²„ ë¡œê·¸ í™•ì¸: docker logs ë˜ëŠ” RUNPOD ë¡œê·¸ ë·°ì–´")
            return False
            
    except Exception as e:
        print_warning(f"Upload ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return False


def calculate_percentile(data: List[float], percentile: float) -> float:
    """í¼ì„¼íƒ€ì¼ ê³„ì‚°"""
    if not data:
        return 0.0
    sorted_data = sorted(data)
    index = int(len(sorted_data) * percentile / 100)
    return sorted_data[min(index, len(sorted_data) - 1)]


def measure_performance(
    endpoint: str,
    payload: Dict[str, Any],
    num_iterations: int = 5,
    warmup_iterations: int = 1,
    timeout: int = 60
) -> Tuple[bool, Optional[Dict[str, Any]]]:
    """
    ì„±ëŠ¥ ì¸¡ì • (ì—¬ëŸ¬ ë²ˆ ë°˜ë³µ ì‹¤í–‰í•˜ì—¬ í†µê³„ ìˆ˜ì§‘)
    
    Returns:
        (ì„±ê³µ ì—¬ë¶€, ì„±ëŠ¥ í†µê³„ ë”•ì…”ë„ˆë¦¬)
    """
    # endpointê°€ ì´ë¯¸ ì „ì²´ URLì¸ì§€ í™•ì¸ (http:// ë˜ëŠ” https://ë¡œ ì‹œì‘)
    if endpoint.startswith(("http://", "https://")):
        url = endpoint
    else:
        url = f"{BASE_URL}{endpoint}"
    latencies = []
    success_count = 0
    error_count = 0
    error_4xx_count = 0
    error_5xx_count = 0
    status_codes = []
    
    # GPU ëª¨ë‹ˆí„° ì´ˆê¸°í™” (ê°€ëŠ¥í•œ ê²½ìš°)
    gpu_monitor = None
    gpu_metrics_before = None
    gpu_metrics_after = None
    if GPU_MONITOR_AVAILABLE:
        try:
            gpu_monitor = GPUMonitor(device_index=0)
            gpu_metrics_before = gpu_monitor.get_metrics()
        except Exception:
            pass
    
    # CPU/ë©”ëª¨ë¦¬ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹œì‘
    cpu_before = psutil.cpu_percent(interval=None)
    mem_before = psutil.virtual_memory()
    
    # ì›Œë°ì—…
    for i in range(warmup_iterations):
        try:
            requests.post(url, json=payload, timeout=timeout)
        except Exception:
            pass
    
    # ì‹¤ì œ ì¸¡ì •
    measurement_start_time = time.perf_counter()
    last_successful_response = None  # ì •í™•ë„ í‰ê°€ë¥¼ ìœ„í•´ ë§ˆì§€ë§‰ ì„±ê³µ ì‘ë‹µ ì €ì¥
    for i in range(num_iterations):
        try:
            start_time = time.perf_counter()
            response = requests.post(url, json=payload, timeout=timeout)
            end_time = time.perf_counter()
            
            latency = end_time - start_time
            status_codes.append(response.status_code)
            
            if response.status_code == 200:
                latencies.append(latency)
                success_count += 1
                try:
                    last_successful_response = response.json()  # ë§ˆì§€ë§‰ ì„±ê³µ ì‘ë‹µ ì €ì¥
                except:
                    pass
            elif 400 <= response.status_code < 500:
                error_4xx_count += 1
                error_count += 1
                # ì²« ë²ˆì§¸ ìš”ì²­ ì‹¤íŒ¨ ì‹œ ìƒì„¸ ì¶œë ¥
                if i == 0:
                    try:
                        error_detail = response.json()
                        detail_msg = error_detail.get('detail', response.text[:200])
                        print_warning(f"ìš”ì²­ {i+1}/{num_iterations} ì‹¤íŒ¨ (4xx): {detail_msg}")
                    except:
                        print_warning(f"ìš”ì²­ {i+1}/{num_iterations} ì‹¤íŒ¨ (4xx): {response.status_code} - {response.text[:200]}")
            elif 500 <= response.status_code < 600:
                error_5xx_count += 1
                error_count += 1
                # ì²« ë²ˆì§¸ ìš”ì²­ ì‹¤íŒ¨ ì‹œ ìƒì„¸ ì¶œë ¥
                if i == 0:
                    try:
                        error_detail = response.json()
                        detail_msg = error_detail.get('detail', response.text[:200])
                        print_warning(f"ìš”ì²­ {i+1}/{num_iterations} ì‹¤íŒ¨ (5xx): {detail_msg}")
                    except:
                        print_warning(f"ìš”ì²­ {i+1}/{num_iterations} ì‹¤íŒ¨ (5xx): {response.status_code} - {response.text[:200]}")
            else:
                error_count += 1
        except requests.exceptions.Timeout:
            error_count += 1
            if i == 0:
                print_error(f"ìš”ì²­ {i+1}/{num_iterations} íƒ€ì„ì•„ì›ƒ (timeout={timeout}ì´ˆ)")
        except requests.exceptions.ConnectionError as e:
            error_count += 1
            if i == 0:
                print_error(f"ìš”ì²­ {i+1}/{num_iterations} ì—°ê²° ì‹¤íŒ¨: {str(e)}")
        except Exception as e:
            error_count += 1
            if i == 0:  # ì²« ë²ˆì§¸ ìš”ì²­ë§Œ ìƒì„¸ ì¶œë ¥
                print_error(f"ìš”ì²­ {i+1}/{num_iterations} ì˜ˆì™¸ ë°œìƒ: {type(e).__name__}: {str(e)}")
    measurement_end_time = time.perf_counter()
    
    # CPU/ë©”ëª¨ë¦¬ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì¢…ë£Œ
    cpu_after = psutil.cpu_percent(interval=None)
    mem_after = psutil.virtual_memory()
    
    # GPU ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì¢…ë£Œ
    if gpu_monitor:
        try:
            gpu_metrics_after = gpu_monitor.get_metrics()
        except Exception:
            pass
    
    if not latencies:
        # ì‹¤íŒ¨ ì›ì¸ ìƒì„¸ ì¶œë ¥
        print_error(f"ì„±ëŠ¥ ì¸¡ì • ì‹¤íŒ¨: ì„±ê³µí•œ ìš”ì²­ì´ ì—†ìŠµë‹ˆë‹¤.")
        if status_codes:
            print_info(f"  ìƒíƒœ ì½”ë“œ ë¶„í¬: {status_codes}")
            print_info(f"  4xx ì˜¤ë¥˜: {error_4xx_count}ê°œ, 5xx ì˜¤ë¥˜: {error_5xx_count}ê°œ")
        else:
            print_info(f"  ëª¨ë“  ìš”ì²­ì´ ì˜ˆì™¸ë¡œ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. (ì´ {error_count}ê°œ)")
        return False, None
    
    # í†µê³„ ê³„ì‚°
    total_time = measurement_end_time - measurement_start_time
    throughput_req_per_sec = len(latencies) / total_time if total_time > 0 else 0
    
    avg_latency = statistics.mean(latencies)
    min_latency = min(latencies)
    max_latency = max(latencies)
    p95_latency = calculate_percentile(latencies, 95)
    p99_latency = calculate_percentile(latencies, 99)
    
    stats = {
        "avg_latency_sec": avg_latency,
        "min_latency_sec": min_latency,
        "max_latency_sec": max_latency,
        "p95_latency_sec": p95_latency,
        "p99_latency_sec": p99_latency,
        "success_count": success_count,
        "error_count": error_count,
        "error_4xx_count": error_4xx_count,
        "error_5xx_count": error_5xx_count,
        "total_iterations": num_iterations,
        "success_rate": (success_count / num_iterations) * 100 if num_iterations > 0 else 0,
        "throughput_req_per_sec": throughput_req_per_sec,
        "total_measurement_time_sec": total_time,
        "last_successful_response": last_successful_response,  # ì •í™•ë„ í‰ê°€ìš©
    }
    
    # CPU/ë©”ëª¨ë¦¬ ë©”íŠ¸ë¦­ ì¶”ê°€
    if cpu_after is not None:
        stats["cpu_usage_percent"] = cpu_after
    if mem_after is not None:
        stats["memory_usage_percent"] = mem_after.percent
        stats["memory_used_mb"] = mem_after.used / (1024 ** 2)
        stats["memory_total_mb"] = mem_after.total / (1024 ** 2)
    
    # GPU ë©”íŠ¸ë¦­ ì¶”ê°€
    if gpu_metrics_after:
        stats["gpu_utilization_percent"] = gpu_metrics_after.get("gpu_util_percent", 0)
        stats["gpu_memory_usage_percent"] = gpu_metrics_after.get("memory_util_percent", 0)
        stats["gpu_memory_used_mb"] = gpu_metrics_after.get("memory_used_mb", 0)
        stats["gpu_memory_total_mb"] = gpu_metrics_after.get("memory_total_mb", 0)
    
    return True, stats


def load_test(
    endpoint: str,
    payload: Dict[str, Any],
    total_requests: int = 100,
    concurrent_users: int = 10,
    timeout: int = 60,
    ramp_up_seconds: int = 0
) -> Tuple[bool, Optional[Dict[str, Any]]]:
    """
    ë¶€í•˜í…ŒìŠ¤íŠ¸ (ë™ì‹œ ìš”ì²­ ì²˜ë¦¬ ëŠ¥ë ¥ ì¸¡ì •)
    
    Args:
        endpoint: API ì—”ë“œí¬ì¸íŠ¸ ê²½ë¡œ
        payload: ìš”ì²­ í˜ì´ë¡œë“œ
        total_requests: ì´ ìš”ì²­ ìˆ˜
        concurrent_users: ë™ì‹œ ì‚¬ìš©ì ìˆ˜ (ë™ì‹œ ì‹¤í–‰í•  ìš”ì²­ ìˆ˜)
        timeout: ìš”ì²­ íƒ€ì„ì•„ì›ƒ (ì´ˆ)
        ramp_up_seconds: ì ì§„ì  ë¶€í•˜ ì¦ê°€ ì‹œê°„ (ì´ˆ, 0ì´ë©´ ì¦‰ì‹œ ì‹œì‘)
    
    Returns:
        (ì„±ê³µ ì—¬ë¶€, ë¶€í•˜í…ŒìŠ¤íŠ¸ í†µê³„ ë”•ì…”ë„ˆë¦¬)
    """
    # endpointê°€ ì´ë¯¸ ì „ì²´ URLì¸ì§€ í™•ì¸
    if endpoint.startswith(("http://", "https://")):
        url = endpoint
    else:
        url = f"{BASE_URL}{endpoint}"
    
    latencies = []
    success_count = 0
    error_count = 0
    error_4xx_count = 0
    error_5xx_count = 0
    status_codes = []
    request_timestamps = []
    
    def make_request(request_id: int) -> Tuple[int, float, int, Optional[Dict[str, Any]]]:
        """ë‹¨ì¼ ìš”ì²­ ì‹¤í–‰"""
        try:
            start_time = time.perf_counter()
            response = requests.post(url, json=payload, timeout=timeout)
            end_time = time.perf_counter()
            
            latency = end_time - start_time
            status_code = response.status_code
            
            result = None
            if response.status_code == 200:
                try:
                    result = response.json()
                except:
                    pass
            
            return request_id, latency, status_code, result
        except Exception as e:
            # ì—ëŸ¬ëŠ” ë‚˜ì¤‘ì— ì§‘ê³„
            return request_id, -1, 0, None
    
    # GPU ëª¨ë‹ˆí„° ì´ˆê¸°í™”
    gpu_monitor = None
    gpu_metrics_before = None
    gpu_metrics_after = None
    if GPU_MONITOR_AVAILABLE:
        try:
            gpu_monitor = GPUMonitor(device_index=0)
            gpu_metrics_before = gpu_monitor.get_metrics()
        except Exception:
            pass
    
    # CPU/ë©”ëª¨ë¦¬ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹œì‘
    cpu_before = psutil.cpu_percent(interval=None)
    mem_before = psutil.virtual_memory()
    
    print_info(f"ë¶€í•˜í…ŒìŠ¤íŠ¸ ì‹œì‘: ì´ {total_requests}ê°œ ìš”ì²­, ë™ì‹œ ì‚¬ìš©ì {concurrent_users}ëª…")
    if ramp_up_seconds > 0:
        print_info(f"ì ì§„ì  ë¶€í•˜ ì¦ê°€: {ramp_up_seconds}ì´ˆ ë™ì•ˆ ë¶€í•˜ ì¦ê°€")
    
    # ë¶€í•˜í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    test_start_time = time.perf_counter()
    
    with ThreadPoolExecutor(max_workers=concurrent_users) as executor:
        # ìš”ì²­ ì œì¶œ
        futures = []
        for i in range(total_requests):
            # ì ì§„ì  ë¶€í•˜ ì¦ê°€ (ramp-up)
            if ramp_up_seconds > 0:
                delay = (ramp_up_seconds / total_requests) * i
                if delay > 0:
                    time.sleep(delay)
            
            future = executor.submit(make_request, i)
            futures.append(future)
        
        # ê²°ê³¼ ìˆ˜ì§‘
        for future in as_completed(futures):
            try:
                request_id, latency, status_code, result = future.result()
                request_timestamps.append(time.perf_counter())
                
                if latency >= 0:
                    latencies.append(latency)
                    status_codes.append(status_code)
                    
                    if status_code == 200:
                        success_count += 1
                    elif 400 <= status_code < 500:
                        error_4xx_count += 1
                        error_count += 1
                    elif 500 <= status_code < 600:
                        error_5xx_count += 1
                        error_count += 1
                    else:
                        error_count += 1
                else:
                    error_count += 1
            except Exception as e:
                error_count += 1
    
    test_end_time = time.perf_counter()
    
    # CPU/ë©”ëª¨ë¦¬ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì¢…ë£Œ
    cpu_after = psutil.cpu_percent(interval=None)
    mem_after = psutil.virtual_memory()
    
    # GPU ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì¢…ë£Œ
    if gpu_monitor:
        try:
            gpu_metrics_after = gpu_monitor.get_metrics()
        except Exception:
            pass
    
    if not latencies:
        print_error(f"ë¶€í•˜í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: ì„±ê³µí•œ ìš”ì²­ì´ ì—†ìŠµë‹ˆë‹¤.")
        if status_codes:
            print_info(f"  ìƒíƒœ ì½”ë“œ ë¶„í¬: {status_codes}")
            print_info(f"  4xx ì˜¤ë¥˜: {error_4xx_count}ê°œ, 5xx ì˜¤ë¥˜: {error_5xx_count}ê°œ")
        return False, None
    
    # í†µê³„ ê³„ì‚°
    total_time = test_end_time - test_start_time
    throughput_req_per_sec = len(latencies) / total_time if total_time > 0 else 0
    
    # ìš”ì²­ ê°„ê²© ê³„ì‚° (RPS ì¸¡ì •ìš©)
    if len(request_timestamps) > 1:
        intervals = [request_timestamps[i] - request_timestamps[i-1] for i in range(1, len(request_timestamps))]
        avg_interval = statistics.mean(intervals) if intervals else 0
        actual_rps = 1.0 / avg_interval if avg_interval > 0 else 0
    else:
        actual_rps = throughput_req_per_sec
    
    avg_latency = statistics.mean(latencies)
    min_latency = min(latencies)
    max_latency = max(latencies)
    p50_latency = calculate_percentile(latencies, 50)
    p95_latency = calculate_percentile(latencies, 95)
    p99_latency = calculate_percentile(latencies, 99)
    
    # ë™ì‹œ ì²˜ë¦¬ ëŠ¥ë ¥ ê³„ì‚°
    if len(request_timestamps) > 1:
        # ì‹œê°„ ìœˆë„ìš°ì—ì„œ ìµœëŒ€ ë™ì‹œ ìš”ì²­ ìˆ˜ ì¶”ì •
        time_window = 1.0  # 1ì´ˆ ìœˆë„ìš°
        max_concurrent = 0
        for ts in request_timestamps:
            window_end = ts + time_window
            concurrent_count = sum(1 for t in request_timestamps if ts <= t < window_end)
            max_concurrent = max(max_concurrent, concurrent_count)
    else:
        max_concurrent = 1
    
    stats = {
        "total_requests": total_requests,
        "concurrent_users": concurrent_users,
        "success_count": success_count,
        "error_count": error_count,
        "error_4xx_count": error_4xx_count,
        "error_5xx_count": error_5xx_count,
        "success_rate": (success_count / total_requests) * 100 if total_requests > 0 else 0,
        "avg_latency_sec": avg_latency,
        "min_latency_sec": min_latency,
        "max_latency_sec": max_latency,
        "p50_latency_sec": p50_latency,
        "p95_latency_sec": p95_latency,
        "p99_latency_sec": p99_latency,
        "throughput_req_per_sec": throughput_req_per_sec,
        "actual_rps": actual_rps,
        "max_concurrent_requests": max_concurrent,
        "total_test_time_sec": total_time,
        "ramp_up_seconds": ramp_up_seconds,
    }
    
    # CPU/ë©”ëª¨ë¦¬ ë©”íŠ¸ë¦­ ì¶”ê°€
    if cpu_after is not None:
        stats["cpu_usage_percent"] = cpu_after
    if mem_after is not None:
        stats["memory_usage_percent"] = mem_after.percent
        stats["memory_used_mb"] = mem_after.used / (1024 ** 2)
        stats["memory_total_mb"] = mem_after.total / (1024 ** 2)
    
    # GPU ë©”íŠ¸ë¦­ ì¶”ê°€
    if gpu_metrics_after:
        stats["gpu_utilization_percent"] = gpu_metrics_after.get("gpu_util_percent", 0)
        stats["gpu_memory_usage_percent"] = gpu_metrics_after.get("memory_util_percent", 0)
        stats["gpu_memory_used_mb"] = gpu_metrics_after.get("memory_used_mb", 0)
        stats["gpu_memory_total_mb"] = gpu_metrics_after.get("memory_total_mb", 0)
    
    return True, stats


def query_metrics_from_db(analysis_type: str, limit: int = 10) -> Optional[Dict[str, Any]]:
    """SQLiteì—ì„œ ìµœê·¼ ë©”íŠ¸ë¦­ ì¡°íšŒ (í™•ì¥ëœ ì§€í‘œ í¬í•¨)"""
    if not Path(METRICS_DB_PATH).exists():
        return None
    
    try:
        conn = sqlite3.connect(METRICS_DB_PATH)
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        # analysis_metrics ì¡°íšŒ (ìµœì†Œ/ìµœëŒ€ í¬í•¨)
        cursor.execute("""
            SELECT 
                AVG(processing_time_ms) as avg_processing_time_ms,
                MIN(processing_time_ms) as min_processing_time_ms,
                MAX(processing_time_ms) as max_processing_time_ms,
                AVG(tokens_used) as avg_tokens_used,
                MIN(tokens_used) as min_tokens_used,
                MAX(tokens_used) as max_tokens_used,
                COUNT(*) as total_requests,
                SUM(error_count) as total_errors,
                (SUM(error_count) * 100.0 / COUNT(*)) as error_rate_percent,
                (COUNT(*) - SUM(error_count)) * 100.0 / COUNT(*) as success_rate_percent
            FROM analysis_metrics
            WHERE analysis_type = ?
            ORDER BY created_at DESC
            LIMIT ?
        """, (analysis_type, limit))
        
        analysis_result = cursor.fetchone()
        
        # vllm_metrics ì¡°íšŒ (TTFT P95/P99 í¬í•¨)
        cursor.execute("""
            SELECT 
                AVG(ttft_ms) as avg_ttft_ms,
                MIN(ttft_ms) as min_ttft_ms,
                MAX(ttft_ms) as max_ttft_ms,
                AVG(tps) as avg_tps,
                AVG(tpot_ms) as avg_tpot_ms,
                COUNT(*) as total_vllm_requests
            FROM vllm_metrics
            WHERE analysis_type = ?
            ORDER BY created_at DESC
            LIMIT ?
        """, (analysis_type, limit))
        
        vllm_result = cursor.fetchone()
        
        # TTFT P95/P99 ê³„ì‚°
        cursor.execute("""
            SELECT ttft_ms
            FROM vllm_metrics
            WHERE analysis_type = ?
            ORDER BY created_at DESC
            LIMIT ?
        """, (analysis_type, limit))
        
        ttft_values = [row[0] for row in cursor.fetchall() if row[0] is not None]
        p95_ttft_ms = None
        p99_ttft_ms = None
        if ttft_values:
            sorted_ttft = sorted(ttft_values)
            p95_index = int(len(sorted_ttft) * 0.95)
            p99_index = int(len(sorted_ttft) * 0.99)
            p95_ttft_ms = sorted_ttft[min(p95_index, len(sorted_ttft) - 1)]
            p99_ttft_ms = sorted_ttft[min(p99_index, len(sorted_ttft) - 1)]
        
        conn.close()
        
        metrics = {}
        if analysis_result:
            metrics.update({
                "avg_processing_time_ms": analysis_result["avg_processing_time_ms"],
                "min_processing_time_ms": analysis_result["min_processing_time_ms"],
                "max_processing_time_ms": analysis_result["max_processing_time_ms"],
                "avg_tokens_used": analysis_result["avg_tokens_used"],
                "min_tokens_used": analysis_result["min_tokens_used"],
                "max_tokens_used": analysis_result["max_tokens_used"],
                "total_requests": analysis_result["total_requests"],
                "total_errors": analysis_result["total_errors"],
                "error_rate_percent": analysis_result["error_rate_percent"],
                "success_rate_percent": analysis_result["success_rate_percent"]
            })
        if vllm_result:
            metrics.update({
                "avg_ttft_ms": vllm_result["avg_ttft_ms"],
                "min_ttft_ms": vllm_result["min_ttft_ms"],
                "max_ttft_ms": vllm_result["max_ttft_ms"],
                "p95_ttft_ms": p95_ttft_ms,
                "p99_ttft_ms": p99_ttft_ms,
                "avg_tps": vllm_result["avg_tps"],
                "avg_tpot_ms": vllm_result["avg_tpot_ms"],
                "total_vllm_requests": vllm_result["total_vllm_requests"]
            })
        
        return metrics if metrics else None
    except Exception as e:
        print_warning(f"ë©”íŠ¸ë¦­ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        return None


def get_goodput_stats() -> Optional[Dict[str, Any]]:
    """Goodput í†µê³„ ì¡°íšŒ"""
    if not METRICS_COLLECTOR_AVAILABLE:
        return None
    
    try:
        metrics = MetricsCollector()
        goodput_stats = metrics.get_goodput_stats()
        return goodput_stats
    except Exception as e:
        print_warning(f"Goodput í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        return None


def evaluate_accuracy(
    analysis_type: str,
    restaurant_id: int,
    api_result: Dict[str, Any],
    ground_truth_path: Optional[str] = None
) -> Optional[Dict[str, Any]]:
    """
    ì •í™•ë„ í‰ê°€ (Ground Truth ë¹„êµ)
    
    Args:
        analysis_type: ë¶„ì„ íƒ€ì… ('sentiment', 'summary', 'strength')
        restaurant_id: ë ˆìŠ¤í† ë‘ ID
        api_result: API í˜¸ì¶œ ê²°ê³¼
        ground_truth_path: Ground Truth íŒŒì¼ ê²½ë¡œ
        
    Returns:
        ì •í™•ë„ ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬ ë˜ëŠ” None
    """
    if not EVALUATION_AVAILABLE:
        return None
    
    if not ground_truth_path or not Path(ground_truth_path).exists():
        return None
    
    try:
        if analysis_type == "sentiment":
            evaluator = SentimentAnalysisEvaluator(
                base_url=BASE_URL,
                ground_truth_path=ground_truth_path
            )
            if not evaluator.ground_truth:
                return None
            
            # Ground Truthì—ì„œ í•´ë‹¹ ë ˆìŠ¤í† ë‘ ì°¾ê¸°
            restaurants = evaluator.ground_truth.get("restaurants", [])
            gt_restaurant = None
            for r in restaurants:
                if r.get("restaurant_id") == restaurant_id:
                    gt_restaurant = r
                    break
            
            if not gt_restaurant:
                return None
            
            # ì •í™•ë„ ê³„ì‚°
            gt_positive_ratio = gt_restaurant.get("positive_ratio", 0)
            gt_negative_ratio = gt_restaurant.get("negative_ratio", 0)
            predicted_positive_ratio = api_result.get("positive_ratio", 0)
            predicted_negative_ratio = api_result.get("negative_ratio", 0)
            
            ratio_error_positive = abs(predicted_positive_ratio - gt_positive_ratio)
            ratio_error_negative = abs(predicted_negative_ratio - gt_negative_ratio)
            
            return {
                "mae_positive_ratio": ratio_error_positive,
                "mae_negative_ratio": ratio_error_negative,
                "avg_ratio_error": (ratio_error_positive + ratio_error_negative) / 2,
                "ground_truth_positive_ratio": gt_positive_ratio,
                "ground_truth_negative_ratio": gt_negative_ratio,
            }
        
        elif analysis_type == "summary":
            evaluator = SummaryEvaluator(
                base_url=BASE_URL,
                ground_truth_path=ground_truth_path
            )
            if not evaluator.ground_truth:
                return None
            
            # Ground Truthì—ì„œ í•´ë‹¹ ë ˆìŠ¤í† ë‘ ì°¾ê¸°
            restaurants = evaluator.ground_truth.get("restaurants", [])
            gt_restaurant = None
            for r in restaurants:
                if r.get("restaurant_id") == restaurant_id:
                    gt_restaurant = r
                    break
            
            if not gt_restaurant:
                return None
            
            # BLEU Score ê³„ì‚°
            predicted_summary = api_result.get("overall_summary", "")
            gt_summary = gt_restaurant.get("overall_summary", "")
            
            if predicted_summary and gt_summary:
                bleu_score = evaluator.calculate_bleu_score(predicted_summary, gt_summary)
                rouge_scores = evaluator.calculate_rouge_scores(predicted_summary, gt_summary)
                
                return {
                    "bleu_score": bleu_score,
                    "rouge1": rouge_scores.get("rouge1", 0),
                    "rouge2": rouge_scores.get("rouge2", 0),
                    "rougeL": rouge_scores.get("rougeL", 0),
                }
        
        elif analysis_type == "strength":
            evaluator = StrengthExtractionEvaluator(
                base_url=BASE_URL,
                ground_truth_path=ground_truth_path
            )
            if not evaluator.ground_truth:
                return None
            
            # Ground Truthì—ì„œ í•´ë‹¹ ë ˆìŠ¤í† ë‘ ì°¾ê¸°
            restaurants = evaluator.ground_truth.get("restaurants", [])
            gt_restaurant = None
            for r in restaurants:
                if r.get("restaurant_id") == restaurant_id:
                    gt_restaurant = r
                    break
            
            if not gt_restaurant:
                return None
            
            # Precision@K, Recall@K ê³„ì‚° (k=1, 3, 5, 10)
            predicted_strengths = api_result.get("strengths", [])
            gt_strengths = gt_restaurant.get("ground_truth_strengths", {})
            gt_all = gt_strengths.get("representative", []) + gt_strengths.get("distinct", [])
            
            if predicted_strengths and gt_all:
                k_values = [1, 3, 5, 10]
                precision_at_k = {}
                recall_at_k = {}
                
                # ê° k ê°’ì— ëŒ€í•´ Precision@k, Recall@k ê³„ì‚°
                for k in k_values:
                    precision_at_k[f"P@{k}"] = evaluator.calculate_precision_at_k(
                        predicted_strengths=predicted_strengths[:k],
                        ground_truth_strengths=gt_all,
                        k=k
                    )
                    recall_at_k[f"R@{k}"] = evaluator.calculate_recall_at_k(
                        predicted_strengths=predicted_strengths[:k],
                        ground_truth_strengths=gt_all,
                        k=k
                    )
                
                coverage = evaluator.calculate_coverage(
                    predicted_strengths=predicted_strengths,
                    ground_truth_strengths=gt_all
                )
                
                # coverageê°€ ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° coverage ê°’ë§Œ ì¶”ì¶œ
                coverage_value = coverage.get("coverage", 0.0) if isinstance(coverage, dict) else coverage
                
                return {
                    "k_values": k_values,
                    "precision_at_k": precision_at_k,
                    "recall_at_k": recall_at_k,
                    "precision_at_5": precision_at_k.get("P@5", 0.0),  # í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€
                    "recall_at_5": recall_at_k.get("R@5", 0.0),  # í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€
                    "coverage": coverage_value,
                }
        
        return None
    except Exception as e:
        print_warning(f"ì •í™•ë„ í‰ê°€ ì‹¤íŒ¨: {str(e)}")
        return None


def test_sentiment_analysis(enable_benchmark: bool = False, num_iterations: int = 5):
    """ê°ì„± ë¶„ì„ í…ŒìŠ¤íŠ¸"""
    print_header("1. ê°ì„± ë¶„ì„ í…ŒìŠ¤íŠ¸")
    
    url = f"{BASE_URL}{API_PREFIX}/sentiment/analyze"
    payload = {
        "restaurant_id": SAMPLE_RESTAURANT_ID,
        "reviews": SAMPLE_REVIEWS
    }
    
    try:
        if enable_benchmark:
            # ì„±ëŠ¥ ì¸¡ì • ëª¨ë“œ
            print_info(f"ì„±ëŠ¥ ì¸¡ì • ëª¨ë“œ: {num_iterations}íšŒ ë°˜ë³µ ì‹¤í–‰ ì¤‘...")
            success, stats = measure_performance(url, payload, num_iterations=num_iterations, warmup_iterations=1)
            api_result = None  # API ê²°ê³¼ ì €ì¥ìš©
            
            if success and stats:
                print_success(f"ê°ì„± ë¶„ì„ ì„±ê³µ (í‰ê·  ì²˜ë¦¬ ì‹œê°„: {stats['avg_latency_sec']:.2f}ì´ˆ)")
                print(f"  - í‰ê·  ì²˜ë¦¬ ì‹œê°„: {stats['avg_latency_sec']:.3f}ì´ˆ")
                print(f"  - P95 ì²˜ë¦¬ ì‹œê°„: {stats['p95_latency_sec']:.3f}ì´ˆ")
                print(f"  - P99 ì²˜ë¦¬ ì‹œê°„: {stats['p99_latency_sec']:.3f}ì´ˆ")
                print(f"  - ìµœì†Œ/ìµœëŒ€: {stats['min_latency_sec']:.3f}ì´ˆ / {stats['max_latency_sec']:.3f}ì´ˆ")
                print(f"  - ì„±ê³µë¥ : {stats['success_rate']:.1f}% ({stats['success_count']}/{stats['total_iterations']})")
                
                # SQLiteì—ì„œ ë©”íŠ¸ë¦­ ì¡°íšŒ
                db_metrics = query_metrics_from_db("sentiment", limit=5)
                if db_metrics:
                    print_info("SQLite ë©”íŠ¸ë¦­ (ìµœê·¼ 5ê°œ ìš”ì²­):")
                    if db_metrics.get("avg_processing_time_ms"):
                        print(f"  - í‰ê·  ì²˜ë¦¬ ì‹œê°„: {db_metrics['avg_processing_time_ms']:.2f}ms")
                    if db_metrics.get("avg_tokens_used"):
                        print(f"  - í‰ê·  í† í° ì‚¬ìš©ëŸ‰: {db_metrics['avg_tokens_used']:.0f} tokens")
                    if db_metrics.get("avg_ttft_ms"):
                        print(f"  - í‰ê·  TTFT: {db_metrics['avg_ttft_ms']:.2f}ms")
                        sla_status = "âœ“" if db_metrics['avg_ttft_ms'] < 2000 else "âœ—"
                        print(f"  - SLA ì¤€ìˆ˜ (TTFT < 2ì´ˆ): {sla_status}")
                    if db_metrics.get("avg_tps"):
                        print(f"  - í‰ê·  TPS: {db_metrics['avg_tps']:.2f} tokens/sec")
                    if db_metrics.get("error_rate_percent"):
                        print(f"  - ì—ëŸ¬ìœ¨: {db_metrics['error_rate_percent']:.2f}%")
                
                # ëª©í‘œê°’ ë¹„êµ (QUANTITATIVE_METRICS.md: í‰ê·  1.2ì´ˆ, P95 3.2ì´ˆ, P99 6.8ì´ˆ)
                target_avg = 1.2
                target_p95 = 3.2
                target_p99 = 6.8
                
                avg_time = stats['avg_latency_sec']
                p95_time = stats['p95_latency_sec']
                p99_time = stats['p99_latency_sec']
                
                targets_met = []
                if avg_time <= target_avg:
                    targets_met.append(f"í‰ê·  ({avg_time:.2f}ì´ˆ â‰¤ {target_avg}ì´ˆ)")
                else:
                    print_warning(f"  âš  í‰ê·  ëª©í‘œ ë¯¸ë‹¬ì„± (ëª©í‘œ: {target_avg}ì´ˆ, ì‹¤ì œ: {avg_time:.2f}ì´ˆ)")
                
                if p95_time <= target_p95:
                    targets_met.append(f"P95 ({p95_time:.2f}ì´ˆ â‰¤ {target_p95}ì´ˆ)")
                else:
                    print_warning(f"  âš  P95 ëª©í‘œ ë¯¸ë‹¬ì„± (ëª©í‘œ: {target_p95}ì´ˆ, ì‹¤ì œ: {p95_time:.2f}ì´ˆ)")
                
                if p99_time <= target_p99:
                    targets_met.append(f"P99 ({p99_time:.2f}ì´ˆ â‰¤ {target_p99}ì´ˆ)")
                else:
                    print_warning(f"  âš  P99 ëª©í‘œ ë¯¸ë‹¬ì„± (ëª©í‘œ: {target_p99}ì´ˆ, ì‹¤ì œ: {p99_time:.2f}ì´ˆ)")
                
                if len(targets_met) == 3:
                    print_success(f"  âœ“ ëª¨ë“  ëª©í‘œ ë‹¬ì„±: {', '.join(targets_met)}")
                
                # ì •í™•ë„ í‰ê°€ (Ground Truth ë¹„êµ, ë²¤ì¹˜ë§ˆí¬ ëª¨ë“œì—ì„œë„ ìˆ˜í–‰)
                accuracy_metrics = None
                if stats.get("last_successful_response"):
                    ground_truth_path = str(project_root / "scripts" / "Ground_truth_sentiment.json")
                    accuracy_metrics = evaluate_accuracy(
                        analysis_type="sentiment",
                        restaurant_id=SAMPLE_RESTAURANT_ID,
                        api_result=stats.get("last_successful_response", {}),
                        ground_truth_path=ground_truth_path
                    )
                    if accuracy_metrics:
                        print_info("ì •í™•ë„ í‰ê°€ (Ground Truth ë¹„êµ):")
                        if accuracy_metrics.get("mae_positive_ratio") is not None:
                            mae_positive = accuracy_metrics['mae_positive_ratio']
                            if isinstance(mae_positive, (int, float)):
                                print(f"  - MAE (Positive Ratio): {float(mae_positive):.2f}%")
                        if accuracy_metrics.get("mae_negative_ratio") is not None:
                            mae_negative = accuracy_metrics['mae_negative_ratio']
                            if isinstance(mae_negative, (int, float)):
                                print(f"  - MAE (Negative Ratio): {float(mae_negative):.2f}%")
                
                # JSON ì €ì¥ìš© ë©”íŠ¸ë¦­ ìˆ˜ì§‘
                test_metrics["ê°ì„± ë¶„ì„"] = {
                    "performance": {
                        "avg_latency_sec": stats.get("avg_latency_sec"),
                        "min_latency_sec": stats.get("min_latency_sec"),
                        "max_latency_sec": stats.get("max_latency_sec"),
                        "p95_latency_sec": stats.get("p95_latency_sec"),
                        "p99_latency_sec": stats.get("p99_latency_sec"),
                        "success_rate": stats.get("success_rate"),
                        "success_count": stats.get("success_count"),
                        "total_iterations": stats.get("total_iterations"),
                        "throughput_req_per_sec": stats.get("throughput_req_per_sec"),
                        "target_avg_sec": target_avg,
                        "target_p95_sec": target_p95,
                        "target_p99_sec": target_p99,
                        "target_avg_achieved": avg_time <= target_avg,
                        "target_p95_achieved": p95_time <= target_p95,
                        "target_p99_achieved": p99_time <= target_p99,
                    },
                    "sqlite_metrics": db_metrics if db_metrics else None,
                    "accuracy": accuracy_metrics if accuracy_metrics else None,
                }
                
                return True
            else:
                print_error("ì„±ëŠ¥ ì¸¡ì • ì‹¤íŒ¨")
                return False
        else:
            # ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ
            start_time = time.time()
            response = requests.post(url, json=payload, timeout=60)
            elapsed_time = time.time() - start_time
            
            if response.status_code == 200:
                data = response.json()
                print_success(f"ê°ì„± ë¶„ì„ ì„±ê³µ (ì†Œìš” ì‹œê°„: {elapsed_time:.2f}ì´ˆ)")
                print(f"  - ê¸ì • ë¹„ìœ¨: {data.get('positive_ratio', 'N/A')}%")
                print(f"  - ë¶€ì • ë¹„ìœ¨: {data.get('negative_ratio', 'N/A')}%")
                print(f"  - ê¸ì • ê°œìˆ˜: {data.get('positive_count', 'N/A')}")
                print(f"  - ë¶€ì • ê°œìˆ˜: {data.get('negative_count', 'N/A')}")
                print(f"  - ì „ì²´ ê°œìˆ˜: {data.get('total_count', 'N/A')}")
                if data.get('debug'):
                    print(f"  - Request ID: {data['debug'].get('request_id', 'N/A')}")
                    print(f"  - ì²˜ë¦¬ ì‹œê°„: {data['debug'].get('processing_time_ms', 'N/A')}ms")
                return True
            else:
                print_error(f"ê°ì„± ë¶„ì„ ì‹¤íŒ¨: {response.status_code}")
                print(f"  ì‘ë‹µ: {response.text[:200]}")
                return False
    except Exception as e:
        print_error(f"ê°ì„± ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return False


def test_sentiment_analysis_batch(enable_benchmark: bool = False, num_iterations: int = 5):
    """ë°°ì¹˜ ê°ì„± ë¶„ì„ í…ŒìŠ¤íŠ¸"""
    print_header("2. ë°°ì¹˜ ê°ì„± ë¶„ì„ í…ŒìŠ¤íŠ¸")
    
    url = f"{BASE_URL}{API_PREFIX}/sentiment/analyze/batch"
    # 10ê°œ ë ˆìŠ¤í† ë‘ ë°°ì¹˜ ìƒì„± (QUANTITATIVE_METRICS.md ìš”êµ¬ì‚¬í•­)
    restaurants_payload = []
    for i in range(10):
        restaurants_payload.append({
            "restaurant_id": SAMPLE_RESTAURANT_ID + i,
            "reviews": SAMPLE_REVIEWS  # ëª¨ë“  ë ˆìŠ¤í† ë‘ì— ë™ì¼í•œ ë¦¬ë·° ì‚¬ìš©
        })
    
    payload = {
        "restaurants": restaurants_payload
    }
    
    try:
        if enable_benchmark:
            # ì„±ëŠ¥ ì¸¡ì • ëª¨ë“œ
            print_info(f"ì„±ëŠ¥ ì¸¡ì • ëª¨ë“œ: {num_iterations}íšŒ ë°˜ë³µ ì‹¤í–‰ ì¤‘...")
            success, stats = measure_performance(url, payload, num_iterations=num_iterations, warmup_iterations=1, timeout=120)
            
            if success and stats:
                print_success(f"ë°°ì¹˜ ê°ì„± ë¶„ì„ ì„±ê³µ (í‰ê·  ì²˜ë¦¬ ì‹œê°„: {stats['avg_latency_sec']:.2f}ì´ˆ)")
                print_info("ë°°ì¹˜ ì²˜ë¦¬ ì‹œê°„ í†µê³„ (10ê°œ ë ˆìŠ¤í† ë‘):")
                print(f"  - í‰ê· : {stats['avg_latency_sec']:.3f}ì´ˆ")
                print(f"  - P95: {stats['p95_latency_sec']:.3f}ì´ˆ")
                print(f"  - P99: {stats['p99_latency_sec']:.3f}ì´ˆ")
                print(f"  - ìµœì†Œ/ìµœëŒ€: {stats['min_latency_sec']:.3f}ì´ˆ / {stats['max_latency_sec']:.3f}ì´ˆ")
                if stats.get("throughput_req_per_sec"):
                    print(f"  - ì²˜ë¦¬ëŸ‰: {stats['throughput_req_per_sec']:.2f} req/s")
                print(f"  - ì„±ê³µë¥ : {stats['success_rate']:.1f}% ({stats['success_count']}/{stats['total_iterations']})")
                
                # SQLiteì—ì„œ ë©”íŠ¸ë¦­ ì¡°íšŒ
                db_metrics = query_metrics_from_db("sentiment", limit=5)
                
                # ëª©í‘œê°’ ë¹„êµ (QUANTITATIVE_METRICS.md: 5-10ì´ˆ)
                target_min = 5.0
                target_max = 10.0
                avg_time = stats['avg_latency_sec']
                if target_min <= avg_time <= target_max:
                    print_success(f"  âœ“ ëª©í‘œ ë²”ìœ„ ë‹¬ì„± ({target_min}-{target_max}ì´ˆ)")
                else:
                    print_warning(f"  âš  ëª©í‘œ ë²”ìœ„ ë¯¸ë‹¬ì„± (ëª©í‘œ: {target_min}-{target_max}ì´ˆ, ì‹¤ì œ: {avg_time:.2f}ì´ˆ)")
                
                # JSON ì €ì¥ìš© ë©”íŠ¸ë¦­ ìˆ˜ì§‘
                test_metrics["ë°°ì¹˜ ê°ì„± ë¶„ì„"] = {
                    "performance": {
                        "avg_latency_sec": stats.get("avg_latency_sec"),
                        "min_latency_sec": stats.get("min_latency_sec"),
                        "max_latency_sec": stats.get("max_latency_sec"),
                        "p95_latency_sec": stats.get("p95_latency_sec"),
                        "p99_latency_sec": stats.get("p99_latency_sec"),
                        "success_rate": stats.get("success_rate"),
                        "success_count": stats.get("success_count"),
                        "total_iterations": stats.get("total_iterations"),
                        "throughput_req_per_sec": stats.get("throughput_req_per_sec"),
                        "target_min_sec": target_min,
                        "target_max_sec": target_max,
                        "target_achieved": target_min <= avg_time <= target_max,
                    },
                    "sqlite_metrics": db_metrics if db_metrics else None,
                    "accuracy": None,  # ë°°ì¹˜ ê°ì„± ë¶„ì„ì€ ì •í™•ë„ í‰ê°€ ì—†ìŒ
                }
                
                return True
            else:
                print_error("ì„±ëŠ¥ ì¸¡ì • ì‹¤íŒ¨")
                return False
        else:
            # ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ
            start_time = time.time()
            response = requests.post(url, json=payload, timeout=120)
            elapsed_time = time.time() - start_time
            
            if response.status_code == 200:
                data = response.json()
                print_success(f"ë°°ì¹˜ ê°ì„± ë¶„ì„ ì„±ê³µ (ì†Œìš” ì‹œê°„: {elapsed_time:.2f}ì´ˆ)")
                print(f"  - ì²˜ë¦¬ëœ ë ˆìŠ¤í† ë‘ ìˆ˜: {len(data.get('results', []))}")
                for result in data.get('results', [])[:5]:  # ìƒìœ„ 5ê°œë§Œ ì¶œë ¥
                    print(f"    ë ˆìŠ¤í† ë‘ {result.get('restaurant_id')}: "
                          f"ê¸ì • {result.get('positive_ratio', 'N/A')}%, "
                          f"ë¶€ì • {result.get('negative_ratio', 'N/A')}%")
                return True
            else:
                print_error(f"ë°°ì¹˜ ê°ì„± ë¶„ì„ ì‹¤íŒ¨: {response.status_code}")
                print(f"  ì‘ë‹µ: {response.text[:200]}")
                return False
    except Exception as e:
        print_error(f"ë°°ì¹˜ ê°ì„± ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return False


def test_summarize(enable_benchmark: bool = False, num_iterations: int = 5):
    """ë¦¬ë·° ìš”ì•½ í…ŒìŠ¤íŠ¸"""
    print_header("3. ë¦¬ë·° ìš”ì•½ í…ŒìŠ¤íŠ¸")
    
    url = f"{BASE_URL}{API_PREFIX}/llm/summarize"
    payload = {
        "restaurant_id": str(SAMPLE_RESTAURANT_ID),
        "positive_query": "ë§›ìˆë‹¤ ì¢‹ë‹¤ ë§Œì¡±",
        "negative_query": "ë§›ì—†ë‹¤ ë³„ë¡œ ë¶ˆë§Œ",
        "limit": 10,
        "min_score": 0.0
    }
    
    try:
        if enable_benchmark:
            # ì„±ëŠ¥ ì¸¡ì • ëª¨ë“œ
            print_info(f"ì„±ëŠ¥ ì¸¡ì • ëª¨ë“œ: {num_iterations}íšŒ ë°˜ë³µ ì‹¤í–‰ ì¤‘...")
            success, stats = measure_performance(url, payload, num_iterations=num_iterations, warmup_iterations=1, timeout=120)
            
            if success and stats:
                print_success(f"ë¦¬ë·° ìš”ì•½ ì„±ê³µ (í‰ê·  ì²˜ë¦¬ ì‹œê°„: {stats['avg_latency_sec']:.2f}ì´ˆ)")
                print_info("ì²˜ë¦¬ ì‹œê°„ í†µê³„:")
                print(f"  - í‰ê· : {stats['avg_latency_sec']:.3f}ì´ˆ")
                print(f"  - P95: {stats['p95_latency_sec']:.3f}ì´ˆ")
                print(f"  - P99: {stats['p99_latency_sec']:.3f}ì´ˆ")
                if stats.get("throughput_req_per_sec"):
                    print(f"  - ì²˜ë¦¬ëŸ‰: {stats['throughput_req_per_sec']:.2f} req/s")
                print(f"  - ì„±ê³µë¥ : {stats['success_rate']:.1f}% ({stats['success_count']}/{stats['total_iterations']})")
                
                # SQLiteì—ì„œ ë©”íŠ¸ë¦­ ì¡°íšŒ
                db_metrics = query_metrics_from_db("summary", limit=5)
                if db_metrics:
                    print_info("SQLite ë©”íŠ¸ë¦­ (ìµœê·¼ 5ê°œ ìš”ì²­):")
                    if db_metrics.get("avg_processing_time_ms"):
                        print(f"  - í‰ê·  ì²˜ë¦¬ ì‹œê°„: {db_metrics['avg_processing_time_ms']:.2f}ms")
                    if db_metrics.get("avg_tokens_used"):
                        print(f"  - í‰ê·  í† í° ì‚¬ìš©ëŸ‰: {db_metrics['avg_tokens_used']:.0f} tokens")
                    if db_metrics.get("avg_ttft_ms"):
                        print(f"  - í‰ê·  TTFT: {db_metrics['avg_ttft_ms']:.2f}ms")
                        if db_metrics.get("p95_ttft_ms"):
                            print(f"  - P95 TTFT: {db_metrics['p95_ttft_ms']:.2f}ms")
                        if db_metrics.get("p99_ttft_ms"):
                            print(f"  - P99 TTFT: {db_metrics['p99_ttft_ms']:.2f}ms")
                        sla_status = "âœ“" if db_metrics['avg_ttft_ms'] < 2000 else "âœ—"
                        print(f"  - SLA ì¤€ìˆ˜ (TTFT < 2ì´ˆ): {sla_status}")
                    if db_metrics.get("avg_tps"):
                        print(f"  - í‰ê·  TPS: {db_metrics['avg_tps']:.2f} tokens/sec")
                
                # ëª©í‘œê°’ ë¹„êµ (QUANTITATIVE_METRICS.md: í‰ê·  2.5ì´ˆ, P95 4.8ì´ˆ, P99 9.5ì´ˆ)
                target_avg = 2.5
                target_p95 = 4.8
                target_p99 = 9.5
                
                avg_time = stats['avg_latency_sec']
                p95_time = stats['p95_latency_sec']
                p99_time = stats['p99_latency_sec']
                
                targets_met = []
                if avg_time <= target_avg:
                    targets_met.append(f"í‰ê·  ({avg_time:.2f}ì´ˆ â‰¤ {target_avg}ì´ˆ)")
                else:
                    print_warning(f"  âš  í‰ê·  ëª©í‘œ ë¯¸ë‹¬ì„± (ëª©í‘œ: {target_avg}ì´ˆ, ì‹¤ì œ: {avg_time:.2f}ì´ˆ)")
                
                if p95_time <= target_p95:
                    targets_met.append(f"P95 ({p95_time:.2f}ì´ˆ â‰¤ {target_p95}ì´ˆ)")
                else:
                    print_warning(f"  âš  P95 ëª©í‘œ ë¯¸ë‹¬ì„± (ëª©í‘œ: {target_p95}ì´ˆ, ì‹¤ì œ: {p95_time:.2f}ì´ˆ)")
                
                if p99_time <= target_p99:
                    targets_met.append(f"P99 ({p99_time:.2f}ì´ˆ â‰¤ {target_p99}ì´ˆ)")
                else:
                    print_warning(f"  âš  P99 ëª©í‘œ ë¯¸ë‹¬ì„± (ëª©í‘œ: {target_p99}ì´ˆ, ì‹¤ì œ: {p99_time:.2f}ì´ˆ)")
                
                if len(targets_met) == 3:
                    print_success(f"  âœ“ ëª¨ë“  ëª©í‘œ ë‹¬ì„±: {', '.join(targets_met)}")
                
                # ì •í™•ë„ í‰ê°€ (Ground Truth ë¹„êµ, ë²¤ì¹˜ë§ˆí¬ ëª¨ë“œì—ì„œë„ ìˆ˜í–‰)
                accuracy_metrics = None
                if stats.get("last_successful_response"):
                    ground_truth_path = str(project_root / "scripts" / "Ground_truth_summary.json")
                    accuracy_metrics = evaluate_accuracy(
                        analysis_type="summary",
                        restaurant_id=SAMPLE_RESTAURANT_ID,
                        api_result=stats.get("last_successful_response", {}),
                        ground_truth_path=ground_truth_path
                    )
                    if accuracy_metrics:
                        print_info("ì •í™•ë„ í‰ê°€ (Ground Truth ë¹„êµ):")
                        if accuracy_metrics.get("bleu_score") is not None:
                            bleu_score = accuracy_metrics['bleu_score']
                            if isinstance(bleu_score, (int, float)):
                                print(f"  - BLEU Score: {float(bleu_score):.4f}")
                        if accuracy_metrics.get("rouge1") is not None:
                            rouge1 = accuracy_metrics['rouge1']
                            if isinstance(rouge1, (int, float)):
                                print(f"  - ROUGE-1: {float(rouge1):.4f}")
                        if accuracy_metrics.get("rouge2") is not None:
                            rouge2 = accuracy_metrics['rouge2']
                            if isinstance(rouge2, (int, float)):
                                print(f"  - ROUGE-2: {float(rouge2):.4f}")
                        if accuracy_metrics.get("rougeL") is not None:
                            rougeL = accuracy_metrics['rougeL']
                            if isinstance(rougeL, (int, float)):
                                print(f"  - ROUGE-L: {float(rougeL):.4f}")
                
                # JSON ì €ì¥ìš© ë©”íŠ¸ë¦­ ìˆ˜ì§‘
                test_metrics["ë¦¬ë·° ìš”ì•½"] = {
                    "performance": {
                        "avg_latency_sec": stats.get("avg_latency_sec"),
                        "min_latency_sec": stats.get("min_latency_sec"),
                        "max_latency_sec": stats.get("max_latency_sec"),
                        "p95_latency_sec": stats.get("p95_latency_sec"),
                        "p99_latency_sec": stats.get("p99_latency_sec"),
                        "success_rate": stats.get("success_rate"),
                        "success_count": stats.get("success_count"),
                        "total_iterations": stats.get("total_iterations"),
                        "throughput_req_per_sec": stats.get("throughput_req_per_sec"),
                        "target_avg_sec": target_avg,
                        "target_p95_sec": target_p95,
                        "target_p99_sec": target_p99,
                        "target_avg_achieved": avg_time <= target_avg,
                        "target_p95_achieved": p95_time <= target_p95,
                        "target_p99_achieved": p99_time <= target_p99,
                    },
                    "sqlite_metrics": db_metrics if db_metrics else None,
                    "accuracy": accuracy_metrics if accuracy_metrics else None,
                }
                
                return True
            else:
                print_error("ì„±ëŠ¥ ì¸¡ì • ì‹¤íŒ¨")
                return False
        else:
            # ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ
            start_time = time.time()
            response = requests.post(url, json=payload, timeout=120)
            elapsed_time = time.time() - start_time
            
            if response.status_code == 200:
                data = response.json()
                print_success(f"ë¦¬ë·° ìš”ì•½ ì„±ê³µ (ì†Œìš” ì‹œê°„: {elapsed_time:.2f}ì´ˆ)")
                print(f"  - ì „ì²´ ìš”ì•½: {data.get('overall_summary', 'N/A')[:100]}...")
                print(f"  - ê¸ì • aspect ìˆ˜: {len(data.get('positive_aspects', []))}")
                print(f"  - ë¶€ì • aspect ìˆ˜: {len(data.get('negative_aspects', []))}")
                print(f"  - ê¸ì • ë¦¬ë·° ìˆ˜: {data.get('positive_count', 'N/A')}")
                print(f"  - ë¶€ì • ë¦¬ë·° ìˆ˜: {data.get('negative_count', 'N/A')}")
                
                # ì •í™•ë„ í‰ê°€ (Ground Truth ë¹„êµ, ê¸°ë³¸ ëª¨ë“œì—ì„œë„ ìˆ˜í–‰)
                ground_truth_path = str(project_root / "scripts" / "Ground_truth_summary.json")
                accuracy_metrics = evaluate_accuracy(
                    analysis_type="summary",
                    restaurant_id=SAMPLE_RESTAURANT_ID,
                    api_result=data,
                    ground_truth_path=ground_truth_path
                )
                if accuracy_metrics:
                    print_info("ì •í™•ë„ í‰ê°€ (Ground Truth ë¹„êµ):")
                    if accuracy_metrics.get("bleu_score") is not None:
                        print(f"  - BLEU Score: {accuracy_metrics['bleu_score']:.4f}")
                
                return True
            else:
                print_error(f"ë¦¬ë·° ìš”ì•½ ì‹¤íŒ¨: {response.status_code}")
                print(f"  ì‘ë‹µ: {response.text[:200]}")
                return False
    except Exception as e:
        print_error(f"ë¦¬ë·° ìš”ì•½ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return False


def test_summarize_batch(enable_benchmark: bool = False, num_iterations: int = 5):
    """ë°°ì¹˜ ë¦¬ë·° ìš”ì•½ í…ŒìŠ¤íŠ¸"""
    print_header("4. ë°°ì¹˜ ë¦¬ë·° ìš”ì•½ í…ŒìŠ¤íŠ¸")
    
    url = f"{BASE_URL}{API_PREFIX}/llm/summarize/batch"
    payload = {
        "restaurants": [
            {
                "restaurant_id": SAMPLE_RESTAURANT_ID,
                "positive_query": "ë§›ìˆë‹¤ ì¢‹ë‹¤ ë§Œì¡±",
                "negative_query": "ë§›ì—†ë‹¤ ë³„ë¡œ ë¶ˆë§Œ",
                "limit": 10,
                "min_score": 0.0
            },
            {
                "restaurant_id": SAMPLE_RESTAURANT_ID + 1,
                "positive_query": "ë§›ìˆë‹¤ ì¢‹ë‹¤ ë§Œì¡±",
                "negative_query": "ë§›ì—†ë‹¤ ë³„ë¡œ ë¶ˆë§Œ",
                "limit": 10,
                "min_score": 0.0
            }
        ]
    }
    
    try:
        if enable_benchmark:
            # ì„±ëŠ¥ ì¸¡ì • ëª¨ë“œ
            print_info(f"ì„±ëŠ¥ ì¸¡ì • ëª¨ë“œ: {num_iterations}íšŒ ë°˜ë³µ ì‹¤í–‰ ì¤‘...")
            success, stats = measure_performance(url, payload, num_iterations=num_iterations, warmup_iterations=1, timeout=180)
            
            if success and stats:
                print_success(f"ë°°ì¹˜ ë¦¬ë·° ìš”ì•½ ì„±ê³µ (í‰ê·  ì²˜ë¦¬ ì‹œê°„: {stats['avg_latency_sec']:.2f}ì´ˆ)")
                print_info("ì²˜ë¦¬ ì‹œê°„ í†µê³„:")
                print(f"  - í‰ê· : {stats['avg_latency_sec']:.3f}ì´ˆ")
                print(f"  - P95: {stats['p95_latency_sec']:.3f}ì´ˆ")
                print(f"  - P99: {stats['p99_latency_sec']:.3f}ì´ˆ")
                if stats.get("throughput_req_per_sec"):
                    print(f"  - ì²˜ë¦¬ëŸ‰: {stats['throughput_req_per_sec']:.2f} req/s")
                print(f"  - ì„±ê³µë¥ : {stats['success_rate']:.1f}% ({stats['success_count']}/{stats['total_iterations']})")
                
                # SQLiteì—ì„œ ë©”íŠ¸ë¦­ ì¡°íšŒ
                db_metrics = query_metrics_from_db("summary", limit=5)
                if db_metrics:
                    print_info("SQLite ë©”íŠ¸ë¦­ (ìµœê·¼ 5ê°œ ìš”ì²­):")
                    if db_metrics.get("avg_processing_time_ms"):
                        print(f"  - í‰ê·  ì²˜ë¦¬ ì‹œê°„: {db_metrics['avg_processing_time_ms']:.2f}ms")
                    if db_metrics.get("avg_tokens_used"):
                        print(f"  - í‰ê·  í† í° ì‚¬ìš©ëŸ‰: {db_metrics['avg_tokens_used']:.0f} tokens")
                    if db_metrics.get("avg_ttft_ms"):
                        print(f"  - í‰ê·  TTFT: {db_metrics['avg_ttft_ms']:.2f}ms")
                        if db_metrics.get("p95_ttft_ms"):
                            print(f"  - P95 TTFT: {db_metrics['p95_ttft_ms']:.2f}ms")
                        if db_metrics.get("p99_ttft_ms"):
                            print(f"  - P99 TTFT: {db_metrics['p99_ttft_ms']:.2f}ms")
                        sla_status = "âœ“" if db_metrics['avg_ttft_ms'] < 2000 else "âœ—"
                        print(f"  - SLA ì¤€ìˆ˜ (TTFT < 2ì´ˆ): {sla_status}")
                    if db_metrics.get("avg_tps"):
                        print(f"  - í‰ê·  TPS: {db_metrics['avg_tps']:.2f} tokens/sec")
                
                # ëª©í‘œê°’ ë¹„êµ (QUANTITATIVE_METRICS.md: ë°°ì¹˜ ì²˜ë¦¬ 5-10ì´ˆ)
                target_min = 5.0
                target_max = 10.0
                avg_time = stats['avg_latency_sec']
                if target_min <= avg_time <= target_max:
                    print_success(f"  âœ“ ëª©í‘œ ë²”ìœ„ ë‹¬ì„± ({target_min}-{target_max}ì´ˆ)")
                else:
                    print_warning(f"  âš  ëª©í‘œ ë²”ìœ„ ë¯¸ë‹¬ì„± (ëª©í‘œ: {target_min}-{target_max}ì´ˆ, ì‹¤ì œ: {avg_time:.2f}ì´ˆ)")
                
                # ì •í™•ë„ í‰ê°€ (Ground Truth ë¹„êµ, ë²¤ì¹˜ë§ˆí¬ ëª¨ë“œì—ì„œë„ ìˆ˜í–‰)
                accuracy_metrics = None
                if stats.get("last_successful_response"):
                    ground_truth_path = str(project_root / "scripts" / "Ground_truth_summary.json")
                    accuracy_metrics = evaluate_accuracy(
                        analysis_type="summary",
                        restaurant_id=SAMPLE_RESTAURANT_ID,
                        api_result=stats.get("last_successful_response", {}),
                        ground_truth_path=ground_truth_path
                    )
                    if accuracy_metrics:
                        print_info("ì •í™•ë„ í‰ê°€ (Ground Truth ë¹„êµ):")
                        if accuracy_metrics.get("bleu_score") is not None:
                            bleu_score = accuracy_metrics['bleu_score']
                            if isinstance(bleu_score, (int, float)):
                                print(f"  - BLEU Score: {float(bleu_score):.4f}")
                        if accuracy_metrics.get("rouge1") is not None:
                            rouge1 = accuracy_metrics['rouge1']
                            if isinstance(rouge1, (int, float)):
                                print(f"  - ROUGE-1: {float(rouge1):.4f}")
                        if accuracy_metrics.get("rouge2") is not None:
                            rouge2 = accuracy_metrics['rouge2']
                            if isinstance(rouge2, (int, float)):
                                print(f"  - ROUGE-2: {float(rouge2):.4f}")
                        if accuracy_metrics.get("rougeL") is not None:
                            rougeL = accuracy_metrics['rougeL']
                            if isinstance(rougeL, (int, float)):
                                print(f"  - ROUGE-L: {float(rougeL):.4f}")
                
                # JSON ì €ì¥ìš© ë©”íŠ¸ë¦­ ìˆ˜ì§‘
                test_metrics["ë°°ì¹˜ ë¦¬ë·° ìš”ì•½"] = {
                    "performance": {
                        "avg_latency_sec": stats.get("avg_latency_sec"),
                        "min_latency_sec": stats.get("min_latency_sec"),
                        "max_latency_sec": stats.get("max_latency_sec"),
                        "p95_latency_sec": stats.get("p95_latency_sec"),
                        "p99_latency_sec": stats.get("p99_latency_sec"),
                        "success_rate": stats.get("success_rate"),
                        "success_count": stats.get("success_count"),
                        "total_iterations": stats.get("total_iterations"),
                        "throughput_req_per_sec": stats.get("throughput_req_per_sec"),
                        "target_min_sec": target_min,
                        "target_max_sec": target_max,
                        "target_achieved": target_min <= avg_time <= target_max,
                    },
                    "sqlite_metrics": db_metrics if db_metrics else None,
                    "accuracy": accuracy_metrics if accuracy_metrics else None,
                }
                
                return True
            else:
                print_error("ì„±ëŠ¥ ì¸¡ì • ì‹¤íŒ¨")
                return False
        else:
            # ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ
            start_time = time.time()
            response = requests.post(url, json=payload, timeout=180)
            elapsed_time = time.time() - start_time
            
            if response.status_code == 200:
                data = response.json()
                print_success(f"ë°°ì¹˜ ë¦¬ë·° ìš”ì•½ ì„±ê³µ (ì†Œìš” ì‹œê°„: {elapsed_time:.2f}ì´ˆ)")
                print(f"  - ì²˜ë¦¬ëœ ë ˆìŠ¤í† ë‘ ìˆ˜: {len(data.get('results', []))}")
                for result in data.get('results', []):
                    print(f"    ë ˆìŠ¤í† ë‘ {result.get('restaurant_id')}: "
                          f"ìš”ì•½ ì™„ë£Œ ({len(result.get('positive_aspects', []))}ê°œ ê¸ì •, "
                          f"{len(result.get('negative_aspects', []))}ê°œ ë¶€ì • aspect)")
                return True
            else:
                print_error(f"ë°°ì¹˜ ë¦¬ë·° ìš”ì•½ ì‹¤íŒ¨: {response.status_code}")
                print(f"  ì‘ë‹µ: {response.text[:200]}")
                return False
    except Exception as e:
        print_error(f"ë°°ì¹˜ ë¦¬ë·° ìš”ì•½ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return False


def test_extract_strengths(enable_benchmark: bool = False, num_iterations: int = 5):
    """ê°•ì  ì¶”ì¶œ í…ŒìŠ¤íŠ¸"""
    print_header("5. ê°•ì  ì¶”ì¶œ í…ŒìŠ¤íŠ¸")
    
    url = f"{BASE_URL}{API_PREFIX}/llm/extract/strengths"
    payload = {
        "restaurant_id": SAMPLE_RESTAURANT_ID,
        "strength_type": "both",  # representative + distinct
        "category_filter": None,  # Noneì´ë©´ ëª¨ë“  ë ˆìŠ¤í† ë‘ê³¼ ë¹„êµ (ë¹„êµêµ° ì°¾ê¸° ê°€ëŠ¥)
        "region_filter": None,
        "price_band_filter": None,
        "top_k": 5,
        "max_candidates": 100,
        "months_back": 24,  # í…ŒìŠ¤íŠ¸ ë°ì´í„° ëŒ€ì‘ì„ ìœ„í•´ 24ê°œì›”ë¡œ í™•ëŒ€
        "min_support": 1  # í…ŒìŠ¤íŠ¸ ë°ì´í„° ëŒ€ì‘ì„ ìœ„í•´ 1ë¡œ ë‚®ì¶¤ (ìµœì†Œ 1ê°œ ë¦¬ë·°ë§Œ ìˆì–´ë„ í†µê³¼)
    }
    
    try:
        if enable_benchmark:
            # ì„±ëŠ¥ ì¸¡ì • ëª¨ë“œ
            print_info(f"ì„±ëŠ¥ ì¸¡ì • ëª¨ë“œ: {num_iterations}íšŒ ë°˜ë³µ ì‹¤í–‰ ì¤‘...")
            success, stats = measure_performance(url, payload, num_iterations=num_iterations, warmup_iterations=1, timeout=180)
            
            if success and stats:
                print_success(f"ê°•ì  ì¶”ì¶œ ì„±ê³µ (í‰ê·  ì²˜ë¦¬ ì‹œê°„: {stats['avg_latency_sec']:.2f}ì´ˆ)")
                print_info("ì²˜ë¦¬ ì‹œê°„ í†µê³„:")
                print(f"  - í‰ê· : {stats['avg_latency_sec']:.3f}ì´ˆ")
                print(f"  - P95: {stats['p95_latency_sec']:.3f}ì´ˆ")
                print(f"  - P99: {stats['p99_latency_sec']:.3f}ì´ˆ")
                if stats.get("throughput_req_per_sec"):
                    print(f"  - ì²˜ë¦¬ëŸ‰: {stats['throughput_req_per_sec']:.2f} req/s")
                print(f"  - ì„±ê³µë¥ : {stats['success_rate']:.1f}% ({stats['success_count']}/{stats['total_iterations']})")
                
                # SQLiteì—ì„œ ë©”íŠ¸ë¦­ ì¡°íšŒ
                db_metrics = query_metrics_from_db("strength", limit=5)
                if db_metrics:
                    print_info("SQLite ë©”íŠ¸ë¦­ (ìµœê·¼ 5ê°œ ìš”ì²­):")
                    if db_metrics.get("avg_processing_time_ms"):
                        print(f"  - í‰ê·  ì²˜ë¦¬ ì‹œê°„: {db_metrics['avg_processing_time_ms']:.2f}ms")
                    if db_metrics.get("avg_tokens_used"):
                        print(f"  - í‰ê·  í† í° ì‚¬ìš©ëŸ‰: {db_metrics['avg_tokens_used']:.0f} tokens")
                    if db_metrics.get("avg_ttft_ms"):
                        print(f"  - í‰ê·  TTFT: {db_metrics['avg_ttft_ms']:.2f}ms")
                        if db_metrics.get("p95_ttft_ms"):
                            print(f"  - P95 TTFT: {db_metrics['p95_ttft_ms']:.2f}ms")
                        if db_metrics.get("p99_ttft_ms"):
                            print(f"  - P99 TTFT: {db_metrics['p99_ttft_ms']:.2f}ms")
                        sla_status = "âœ“" if db_metrics['avg_ttft_ms'] < 2000 else "âœ—"
                        print(f"  - SLA ì¤€ìˆ˜ (TTFT < 2ì´ˆ): {sla_status}")
                    if db_metrics.get("avg_tps"):
                        print(f"  - í‰ê·  TPS: {db_metrics['avg_tps']:.2f} tokens/sec")
                
                # ëª©í‘œê°’ ë¹„êµ (QUANTITATIVE_METRICS.md: í‰ê·  3.0ì´ˆ, P95 5.5ì´ˆ, P99 11.2ì´ˆ)
                target_avg = 3.0
                target_p95 = 5.5
                target_p99 = 11.2
                
                avg_time = stats['avg_latency_sec']
                p95_time = stats['p95_latency_sec']
                p99_time = stats['p99_latency_sec']
                
                targets_met = []
                if avg_time <= target_avg:
                    targets_met.append(f"í‰ê·  ({avg_time:.2f}ì´ˆ â‰¤ {target_avg}ì´ˆ)")
                else:
                    print_warning(f"  âš  í‰ê·  ëª©í‘œ ë¯¸ë‹¬ì„± (ëª©í‘œ: {target_avg}ì´ˆ, ì‹¤ì œ: {avg_time:.2f}ì´ˆ)")
                
                if p95_time <= target_p95:
                    targets_met.append(f"P95 ({p95_time:.2f}ì´ˆ â‰¤ {target_p95}ì´ˆ)")
                else:
                    print_warning(f"  âš  P95 ëª©í‘œ ë¯¸ë‹¬ì„± (ëª©í‘œ: {target_p95}ì´ˆ, ì‹¤ì œ: {p95_time:.2f}ì´ˆ)")
                
                if p99_time <= target_p99:
                    targets_met.append(f"P99 ({p99_time:.2f}ì´ˆ â‰¤ {target_p99}ì´ˆ)")
                else:
                    print_warning(f"  âš  P99 ëª©í‘œ ë¯¸ë‹¬ì„± (ëª©í‘œ: {target_p99}ì´ˆ, ì‹¤ì œ: {p99_time:.2f}ì´ˆ)")
                
                if len(targets_met) == 3:
                    print_success(f"  âœ“ ëª¨ë“  ëª©í‘œ ë‹¬ì„±: {', '.join(targets_met)}")
                
                # ì •í™•ë„ í‰ê°€ (Ground Truth ë¹„êµ)
                accuracy_metrics = None
                ground_truth_path = str(project_root / "scripts" / "Ground_truth_strength.json")
                accuracy_metrics = evaluate_accuracy(
                    analysis_type="strength",
                    restaurant_id=SAMPLE_RESTAURANT_ID,
                    api_result=stats.get("last_successful_response", {}),
                    ground_truth_path=ground_truth_path
                )
                if accuracy_metrics:
                    print_info("ì •í™•ë„ í‰ê°€ (Ground Truth ë¹„êµ):")
                    
                    # k_values ì „ì²´ì— ëŒ€í•œ Precision/Recall ì¶œë ¥
                    if accuracy_metrics.get("precision_at_k"):
                        print_info("  Precision@k:")
                        precision_at_k = accuracy_metrics.get("precision_at_k", {})
                        k_values = accuracy_metrics.get("k_values", [1, 3, 5, 10])
                        for k in k_values:
                            k_key = f"P@{k}"
                            precision = precision_at_k.get(k_key, 0.0)
                            if isinstance(precision, (int, float)):
                                print(f"    - {k_key}: {float(precision):.4f} ({float(precision)*100:.2f}%)")
                    
                    if accuracy_metrics.get("recall_at_k"):
                        print_info("  Recall@k:")
                        recall_at_k = accuracy_metrics.get("recall_at_k", {})
                        k_values = accuracy_metrics.get("k_values", [1, 3, 5, 10])
                        for k in k_values:
                            k_key = f"R@{k}"
                            recall = recall_at_k.get(k_key, 0.0)
                            if isinstance(recall, (int, float)):
                                print(f"    - {k_key}: {float(recall):.4f} ({float(recall)*100:.2f}%)")
                    
                    # í•˜ìœ„ í˜¸í™˜ì„±: precision_at_5, recall_at_5 ê°œë³„ ì¶œë ¥ë„ ì§€ì›
                    if accuracy_metrics.get("precision_at_5") is not None and not accuracy_metrics.get("precision_at_k"):
                        precision_at_5 = accuracy_metrics['precision_at_5']
                        if isinstance(precision_at_5, (int, float)):
                            print(f"  - Precision@5: {float(precision_at_5):.4f}")
                    if accuracy_metrics.get("recall_at_5") is not None and not accuracy_metrics.get("recall_at_k"):
                        recall_at_5 = accuracy_metrics["recall_at_5"]
                        if isinstance(recall_at_5, (int, float)):
                            print(f"  - Recall@5: {float(recall_at_5):.4f}")
                    
                    if accuracy_metrics.get("coverage") is not None:
                        coverage = accuracy_metrics['coverage']
                        # coverageê°€ ë”•ì…”ë„ˆë¦¬ì¼ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ìˆ«ìë¡œ ë³€í™˜
                        if isinstance(coverage, (int, float)):
                            print(f"  - Coverage: {float(coverage):.4f}")
                        elif isinstance(coverage, dict):
                            # coverageê°€ ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° (calculate_coverage ë°˜í™˜ê°’)
                            coverage_value = coverage.get("coverage", 0.0)
                            if isinstance(coverage_value, (int, float)):
                                print(f"  - Coverage: {float(coverage_value):.4f}")
                    
                    target_accuracy = 0.88
                    # precision_at_kì—ì„œ P@5 ê°’ì„ ìš°ì„  ì‚¬ìš©
                    precision_at_5_value = accuracy_metrics.get("precision_at_k", {}).get("P@5") or accuracy_metrics.get("precision_at_5", 0)
                    if isinstance(precision_at_5_value, (int, float)) and float(precision_at_5_value) >= target_accuracy:
                        print_success(f"  âœ“ ëª©í‘œ ë‹¬ì„± (ëª©í‘œ: {target_accuracy}, ì‹¤ì œ: {float(precision_at_5_value):.4f})")
                    elif isinstance(precision_at_5_value, (int, float)):
                        print_warning(f"  âš  ëª©í‘œ ë¯¸ë‹¬ì„± (ëª©í‘œ: {target_accuracy}, ì‹¤ì œ: {float(precision_at_5_value):.4f})")
                
                # JSON ì €ì¥ìš© ë©”íŠ¸ë¦­ ìˆ˜ì§‘
                test_metrics["ê°•ì  ì¶”ì¶œ"] = {
                    "performance": {
                        "avg_latency_sec": stats.get("avg_latency_sec"),
                        "min_latency_sec": stats.get("min_latency_sec"),
                        "max_latency_sec": stats.get("max_latency_sec"),
                        "p95_latency_sec": stats.get("p95_latency_sec"),
                        "p99_latency_sec": stats.get("p99_latency_sec"),
                        "success_rate": stats.get("success_rate"),
                        "success_count": stats.get("success_count"),
                        "total_iterations": stats.get("total_iterations"),
                        "throughput_req_per_sec": stats.get("throughput_req_per_sec"),
                        "target_avg_sec": target_avg,
                        "target_p95_sec": target_p95,
                        "target_p99_sec": target_p99,
                        "target_avg_achieved": avg_time <= target_avg,
                        "target_p95_achieved": p95_time <= target_p95,
                        "target_p99_achieved": p99_time <= target_p99,
                    },
                    "sqlite_metrics": db_metrics if db_metrics else None,
                    "accuracy": accuracy_metrics if accuracy_metrics else None,
                }
                
                return True
            else:
                print_error("ì„±ëŠ¥ ì¸¡ì • ì‹¤íŒ¨")
                return False
        else:
            # ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ
            start_time = time.time()
            response = requests.post(url, json=payload, timeout=180)
            elapsed_time = time.time() - start_time
            
            if response.status_code == 200:
                data = response.json()
                print_success(f"ê°•ì  ì¶”ì¶œ ì„±ê³µ (ì†Œìš” ì‹œê°„: {elapsed_time:.2f}ì´ˆ)")
                print(f"  - ì¶”ì¶œëœ ê°•ì  ìˆ˜: {len(data.get('strengths', []))}")
                print(f"  - í›„ë³´ ìˆ˜: {data.get('total_candidates', 'N/A')}")
                print(f"  - ê²€ì¦ í†µê³¼ ìˆ˜: {data.get('validated_count', 'N/A')}")
                
                # ìƒìœ„ 3ê°œ ê°•ì  ì¶œë ¥
                for i, strength in enumerate(data.get('strengths', [])[:3], 1):
                    print(f"\n  ê°•ì  {i}:")
                    print(f"    - Aspect: {strength.get('aspect', 'N/A')}")
                    print(f"    - Claim: {strength.get('claim', 'N/A')[:50]}...")
                    print(f"    - Support Count: {strength.get('support_count', 'N/A')}")
                    if strength.get('distinct_score') is not None:
                        print(f"    - Distinct Score: {strength.get('distinct_score', 'N/A')}")
                
                # ì •í™•ë„ í‰ê°€ (Ground Truth ë¹„êµ, ê¸°ë³¸ ëª¨ë“œì—ì„œë„ ìˆ˜í–‰)
                ground_truth_path = str(project_root / "scripts" / "Ground_truth_strength.json")
                accuracy_metrics = evaluate_accuracy(
                    analysis_type="strength",
                    restaurant_id=SAMPLE_RESTAURANT_ID,
                    api_result=data,
                    ground_truth_path=ground_truth_path
                )
                if accuracy_metrics:
                    print_info("ì •í™•ë„ í‰ê°€ (Ground Truth ë¹„êµ):")
                    
                    # k_values ì „ì²´ì— ëŒ€í•œ Precision/Recall ì¶œë ¥
                    if accuracy_metrics.get("precision_at_k"):
                        print_info("  Precision@k:")
                        precision_at_k = accuracy_metrics.get("precision_at_k", {})
                        k_values = accuracy_metrics.get("k_values", [1, 3, 5, 10])
                        for k in k_values:
                            k_key = f"P@{k}"
                            precision = precision_at_k.get(k_key, 0.0)
                            if isinstance(precision, (int, float)):
                                print(f"    - {k_key}: {float(precision):.4f} ({float(precision)*100:.2f}%)")
                    
                    if accuracy_metrics.get("recall_at_k"):
                        print_info("  Recall@k:")
                        recall_at_k = accuracy_metrics.get("recall_at_k", {})
                        k_values = accuracy_metrics.get("k_values", [1, 3, 5, 10])
                        for k in k_values:
                            k_key = f"R@{k}"
                            recall = recall_at_k.get(k_key, 0.0)
                            if isinstance(recall, (int, float)):
                                print(f"    - {k_key}: {float(recall):.4f} ({float(recall)*100:.2f}%)")
                    
                    # í•˜ìœ„ í˜¸í™˜ì„±: precision_at_5, recall_at_5 ê°œë³„ ì¶œë ¥ë„ ì§€ì›
                    if accuracy_metrics.get("precision_at_5") is not None and not accuracy_metrics.get("precision_at_k"):
                        precision_at_5 = accuracy_metrics['precision_at_5']
                        if isinstance(precision_at_5, (int, float)):
                            print(f"  - Precision@5: {float(precision_at_5):.4f}")
                    if accuracy_metrics.get("recall_at_5") is not None and not accuracy_metrics.get("recall_at_k"):
                        recall_at_5 = accuracy_metrics["recall_at_5"]
                        if isinstance(recall_at_5, (int, float)):
                            print(f"  - Recall@5: {float(recall_at_5):.4f}")
                    
                    if accuracy_metrics.get("coverage") is not None:
                        coverage = accuracy_metrics['coverage']
                        # coverageê°€ ë”•ì…”ë„ˆë¦¬ì¼ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ìˆ«ìë¡œ ë³€í™˜
                        if isinstance(coverage, (int, float)):
                            print(f"  - Coverage: {float(coverage):.4f}")
                        elif isinstance(coverage, dict):
                            # coverageê°€ ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° (calculate_coverage ë°˜í™˜ê°’)
                            coverage_value = coverage.get("coverage", 0.0)
                            if isinstance(coverage_value, (int, float)):
                                print(f"  - Coverage: {float(coverage_value):.4f}")
                
                return True
            else:
                print_error(f"ê°•ì  ì¶”ì¶œ ì‹¤íŒ¨: {response.status_code}")
                print(f"  ì‘ë‹µ: {response.text[:200]}")
                return False
    except Exception as e:
        print_error(f"ê°•ì  ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return False


def test_vector_search(enable_benchmark: bool = False, num_iterations: int = 5):
    """ë²¡í„° ê²€ìƒ‰ í…ŒìŠ¤íŠ¸"""
    print_header("6. ë²¡í„° ê²€ìƒ‰ í…ŒìŠ¤íŠ¸")
    
    url = f"{BASE_URL}{API_PREFIX}/vector/search/similar"
    payload = {
        "query_text": "ë§›ìˆë‹¤ ì¢‹ë‹¤ ë§Œì¡±",
        "restaurant_id": SAMPLE_RESTAURANT_ID,
        "limit": 5,
        "min_score": 0.0
    }
    
    try:
        if enable_benchmark:
            # ì„±ëŠ¥ ì¸¡ì • ëª¨ë“œ
            print_info(f"ì„±ëŠ¥ ì¸¡ì • ëª¨ë“œ: {num_iterations}íšŒ ë°˜ë³µ ì‹¤í–‰ ì¤‘...")
            success, stats = measure_performance(url, payload, num_iterations=num_iterations, warmup_iterations=1, timeout=30)
            
            if success and stats:
                print_success(f"ë²¡í„° ê²€ìƒ‰ ì„±ê³µ (í‰ê·  ì²˜ë¦¬ ì‹œê°„: {stats['avg_latency_sec']:.2f}ì´ˆ)")
                print_info("ì²˜ë¦¬ ì‹œê°„ í†µê³„:")
                print(f"  - í‰ê· : {stats['avg_latency_sec']:.3f}ì´ˆ")
                print(f"  - P95: {stats['p95_latency_sec']:.3f}ì´ˆ")
                print(f"  - P99: {stats['p99_latency_sec']:.3f}ì´ˆ")
                if stats.get("throughput_req_per_sec"):
                    print(f"  - ì²˜ë¦¬ëŸ‰: {stats['throughput_req_per_sec']:.2f} req/s")
                print(f"  - ì„±ê³µë¥ : {stats['success_rate']:.1f}% ({stats['success_count']}/{stats['total_iterations']})")
                
                # ëª©í‘œê°’ ë¹„êµ (QUANTITATIVE_METRICS.md: í‰ê·  1.5ì´ˆ, P95 3.0ì´ˆ, P99 6.0ì´ˆ)
                target_avg = 1.5
                target_p95 = 3.0
                target_p99 = 6.0
                
                avg_time = stats['avg_latency_sec']
                p95_time = stats['p95_latency_sec']
                p99_time = stats['p99_latency_sec']
                
                targets_met = []
                if avg_time <= target_avg:
                    targets_met.append(f"í‰ê·  ({avg_time:.2f}ì´ˆ â‰¤ {target_avg}ì´ˆ)")
                else:
                    print_warning(f"  âš  í‰ê·  ëª©í‘œ ë¯¸ë‹¬ì„± (ëª©í‘œ: {target_avg}ì´ˆ, ì‹¤ì œ: {avg_time:.2f}ì´ˆ)")
                
                if p95_time <= target_p95:
                    targets_met.append(f"P95 ({p95_time:.2f}ì´ˆ â‰¤ {target_p95}ì´ˆ)")
                else:
                    print_warning(f"  âš  P95 ëª©í‘œ ë¯¸ë‹¬ì„± (ëª©í‘œ: {target_p95}ì´ˆ, ì‹¤ì œ: {p95_time:.2f}ì´ˆ)")
                
                if p99_time <= target_p99:
                    targets_met.append(f"P99 ({p99_time:.2f}ì´ˆ â‰¤ {target_p99}ì´ˆ)")
                else:
                    print_warning(f"  âš  P99 ëª©í‘œ ë¯¸ë‹¬ì„± (ëª©í‘œ: {target_p99}ì´ˆ, ì‹¤ì œ: {p99_time:.2f}ì´ˆ)")
                
                if len(targets_met) == 3:
                    print_success(f"  âœ“ ëª¨ë“  ëª©í‘œ ë‹¬ì„±: {', '.join(targets_met)}")
                
                # Precision@k í‰ê°€ (ì„ë² ë”© ëª¨ë¸ ì •í™•ë„ ì¸¡ì •)
                precision_metrics = None
                if EVALUATION_AVAILABLE:
                    try:
                        ground_truth_path = str(project_root / "scripts" / "Ground_truth_vector_search.json")
                        if Path(ground_truth_path).exists():
                            evaluator = PrecisionAtKEvaluator(
                                base_url=BASE_URL,
                                ground_truth_path=ground_truth_path
                            )
                            
                            # Precision@k í‰ê°€ ìˆ˜í–‰ (k=1, 3, 5, 10)
                            k_values = [1, 3, 5, 10]
                            precision_result = evaluator.evaluate(
                                k_values=k_values,
                                limit=10,
                                min_score=0.0
                            )
                            
                            if precision_result:
                                avg_precisions = precision_result.get("average_precisions", {})
                                if avg_precisions:
                                    print_info("Precision@k í‰ê°€ (ì„ë² ë”© ëª¨ë¸ ì •í™•ë„):")
                                    for k in k_values:
                                        k_key = f"P@{k}"
                                        precision = avg_precisions.get(k_key, 0.0)
                                        if isinstance(precision, (int, float)):
                                            print(f"  - {k_key}: {float(precision):.4f} ({float(precision)*100:.2f}%)")
                                    
                                    avg_recalls = precision_result.get("average_recalls", {})
                                    if avg_recalls:
                                        print_info("Recall@k í‰ê°€ (ì„ë² ë”© ëª¨ë¸ ì •í™•ë„):")
                                        for k in k_values:
                                            k_key = f"R@{k}"
                                            recall = avg_recalls.get(k_key, 0.0)
                                            if isinstance(recall, (int, float)):
                                                print(f"  - {k_key}: {float(recall):.4f} ({float(recall)*100:.2f}%)")
                                    
                                    precision_metrics = {
                                        "k_values": k_values,
                                        "average_precisions": avg_precisions,
                                        "average_recalls": avg_recalls if avg_recalls else None,
                                        "total_queries": precision_result.get("total_queries", 0),
                                        "evaluated_queries": precision_result.get("evaluated_queries", 0),
                                    }
                        else:
                            print_warning(f"Ground Truth íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {ground_truth_path}")
                    except Exception as e:
                        print_warning(f"Precision@k í‰ê°€ ì‹¤íŒ¨: {str(e)}")
                
                # JSON ì €ì¥ìš© ë©”íŠ¸ë¦­ ìˆ˜ì§‘
                test_metrics["ë²¡í„° ê²€ìƒ‰"] = {
                    "performance": {
                        "avg_latency_sec": stats.get("avg_latency_sec"),
                        "min_latency_sec": stats.get("min_latency_sec"),
                        "max_latency_sec": stats.get("max_latency_sec"),
                        "p95_latency_sec": stats.get("p95_latency_sec"),
                        "p99_latency_sec": stats.get("p99_latency_sec"),
                        "success_rate": stats.get("success_rate"),
                        "success_count": stats.get("success_count"),
                        "total_iterations": stats.get("total_iterations"),
                        "throughput_req_per_sec": stats.get("throughput_req_per_sec"),
                        "target_avg_sec": target_avg,
                        "target_p95_sec": target_p95,
                        "target_p99_sec": target_p99,
                        "target_avg_achieved": avg_time <= target_avg,
                        "target_p95_achieved": p95_time <= target_p95,
                        "target_p99_achieved": p99_time <= target_p99,
                    },
                    "sqlite_metrics": None,
                    "accuracy": precision_metrics if precision_metrics else None,  # Precision@k ë©”íŠ¸ë¦­
                }
                
                return True
            else:
                print_error("ì„±ëŠ¥ ì¸¡ì • ì‹¤íŒ¨")
                return False
        else:
            # ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ
            start_time = time.time()
            response = requests.post(url, json=payload, timeout=30)
            elapsed_time = time.time() - start_time
            
            if response.status_code == 200:
                data = response.json()
                print_success(f"ë²¡í„° ê²€ìƒ‰ ì„±ê³µ (ì†Œìš” ì‹œê°„: {elapsed_time:.2f}ì´ˆ)")
                print(f"  - ê²€ìƒ‰ ê²°ê³¼ ìˆ˜: {len(data.get('results', []))}")
                for i, result in enumerate(data.get('results', [])[:3], 1):
                    print(f"  ê²°ê³¼ {i}:")
                    score = result.get('score', 'N/A')
                    if isinstance(score, (int, float)):
                        print(f"    - ìœ ì‚¬ë„: {score:.3f}")
                    else:
                        print(f"    - ìœ ì‚¬ë„: {score}")
                    # VectorSearchResult êµ¬ì¡°: {"review": {...}, "score": ...}
                    review = result.get('review', {})
                    content = review.get('content', 'N/A')
                    if isinstance(content, str) and len(content) > 50:
                        print(f"    - ë¦¬ë·° ë‚´ìš©: {content[:50]}...")
                    else:
                        print(f"    - ë¦¬ë·° ë‚´ìš©: {content}")
                
                # JSON ì €ì¥ìš© ë©”íŠ¸ë¦­ ìˆ˜ì§‘
                test_metrics["ë²¡í„° ê²€ìƒ‰"] = {
                    "performance": {
                        "elapsed_time_sec": elapsed_time,
                        "result_count": len(data.get('results', [])),
                    },
                    "sqlite_metrics": None,
                    "accuracy": None,
                }
                
                return True
            else:
                print_error(f"ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨: {response.status_code}")
                print(f"  ì‘ë‹µ: {response.text[:200]}")
                return False
    except Exception as e:
        print_error(f"ë²¡í„° ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return False


def test_review_image_search(enable_benchmark: bool = False, num_iterations: int = 5):
    """ë¦¬ë·° ì´ë¯¸ì§€ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸"""
    print_header("7. ë¦¬ë·° ì´ë¯¸ì§€ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸")
    
    url = f"{BASE_URL}{API_PREFIX}/vector/search/review-images"
    payload = {
        "query": "ë§›ìˆë‹¤ ì¢‹ë‹¤ ë§Œì¡±",
        "restaurant_id": SAMPLE_RESTAURANT_ID,
        "limit": 5,
        "min_score": 0.0,
        "expand_query": None  # ìë™ íŒë‹¨
    }
    
    try:
        if enable_benchmark:
            # ì„±ëŠ¥ ì¸¡ì • ëª¨ë“œ
            print_info(f"ì„±ëŠ¥ ì¸¡ì • ëª¨ë“œ: {num_iterations}íšŒ ë°˜ë³µ ì‹¤í–‰ ì¤‘...")
            success, stats = measure_performance(url, payload, num_iterations=num_iterations, warmup_iterations=1, timeout=60)
            
            if success and stats:
                print_success(f"ë¦¬ë·° ì´ë¯¸ì§€ ê²€ìƒ‰ ì„±ê³µ (í‰ê·  ì²˜ë¦¬ ì‹œê°„: {stats['avg_latency_sec']:.2f}ì´ˆ)")
                print_info("ì²˜ë¦¬ ì‹œê°„ í†µê³„:")
                print(f"  - í‰ê· : {stats['avg_latency_sec']:.3f}ì´ˆ")
                print(f"  - P95: {stats['p95_latency_sec']:.3f}ì´ˆ")
                print(f"  - P99: {stats['p99_latency_sec']:.3f}ì´ˆ")
                if stats.get("throughput_req_per_sec"):
                    print(f"  - ì²˜ë¦¬ëŸ‰: {stats['throughput_req_per_sec']:.2f} req/s")
                print(f"  - ì„±ê³µë¥ : {stats['success_rate']:.1f}% ({stats['success_count']}/{stats['total_iterations']})")
                
                # ëª©í‘œê°’ ë¹„êµ (QUANTITATIVE_METRICS.md: í‰ê·  2.0ì´ˆ, P95 4.0ì´ˆ, P99 8.0ì´ˆ)
                target_avg = 2.0
                target_p95 = 4.0
                target_p99 = 8.0
                
                avg_time = stats['avg_latency_sec']
                p95_time = stats['p95_latency_sec']
                p99_time = stats['p99_latency_sec']
                
                targets_met = []
                if avg_time <= target_avg:
                    targets_met.append(f"í‰ê·  ({avg_time:.2f}ì´ˆ â‰¤ {target_avg}ì´ˆ)")
                else:
                    print_warning(f"  âš  í‰ê·  ëª©í‘œ ë¯¸ë‹¬ì„± (ëª©í‘œ: {target_avg}ì´ˆ, ì‹¤ì œ: {avg_time:.2f}ì´ˆ)")
                
                if p95_time <= target_p95:
                    targets_met.append(f"P95 ({p95_time:.2f}ì´ˆ â‰¤ {target_p95}ì´ˆ)")
                else:
                    print_warning(f"  âš  P95 ëª©í‘œ ë¯¸ë‹¬ì„± (ëª©í‘œ: {target_p95}ì´ˆ, ì‹¤ì œ: {p95_time:.2f}ì´ˆ)")
                
                if p99_time <= target_p99:
                    targets_met.append(f"P99 ({p99_time:.2f}ì´ˆ â‰¤ {target_p99}ì´ˆ)")
                else:
                    print_warning(f"  âš  P99 ëª©í‘œ ë¯¸ë‹¬ì„± (ëª©í‘œ: {target_p99}ì´ˆ, ì‹¤ì œ: {p99_time:.2f}ì´ˆ)")
                
                if len(targets_met) == 3:
                    print_success(f"  âœ“ ëª¨ë“  ëª©í‘œ ë‹¬ì„±: {', '.join(targets_met)}")
                
                # SQLiteì—ì„œ ë©”íŠ¸ë¦­ ì¡°íšŒ
                db_metrics = query_metrics_from_db("image_search", limit=5)
                if db_metrics:
                    print_info("SQLite ë©”íŠ¸ë¦­ (ìµœê·¼ 5ê°œ ìš”ì²­):")
                    if db_metrics.get("avg_processing_time_ms"):
                        print(f"  - í‰ê·  ì²˜ë¦¬ ì‹œê°„: {db_metrics['avg_processing_time_ms']:.2f}ms")
                    if db_metrics.get("avg_tokens_used"):
                        print(f"  - í‰ê·  í† í° ì‚¬ìš©ëŸ‰: {db_metrics['avg_tokens_used']:.0f} tokens")
                
                # Precision@k / Recall@k í‰ê°€ (ì„ë² ë”© ëª¨ë¸ ì •í™•ë„ ì¸¡ì •)
                precision_metrics = None
                if EVALUATION_AVAILABLE:
                    try:
                        ground_truth_path = str(project_root / "scripts" / "Ground_truth_vector_search.json")
                        if Path(ground_truth_path).exists():
                            evaluator = PrecisionAtKEvaluator(
                                base_url=BASE_URL,
                                ground_truth_path=ground_truth_path
                            )
                            
                            # ì´ë¯¸ì§€ ê²€ìƒ‰ ê²°ê³¼ì—ì„œ review_id ì¶”ì¶œ
                            last_response = stats.get("last_successful_response", {})
                            if last_response and last_response.get("results"):
                                # ì´ë¯¸ì§€ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë²¡í„° ê²€ìƒ‰ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ í‰ê°€
                                # ì¿¼ë¦¬ì™€ ë ˆìŠ¤í† ë‘ IDë¥¼ ì‚¬ìš©í•˜ì—¬ Ground Truthì™€ ë§¤ì¹­
                                query_text = payload.get("query", "")
                                restaurant_id = payload.get("restaurant_id")
                                
                                # Precision@k í‰ê°€ ìˆ˜í–‰ (k=1, 3, 5, 10)
                                k_values = [1, 3, 5, 10]
                                
                                # ì´ë¯¸ì§€ ê²€ìƒ‰ ê²°ê³¼ì—ì„œ review_id ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ
                                retrieved_review_ids = []
                                for result in last_response.get("results", []):
                                    review_id = result.get("review_id")
                                    if review_id is not None:
                                        try:
                                            retrieved_review_ids.append(int(review_id))
                                        except (ValueError, TypeError):
                                            continue
                                
                                if retrieved_review_ids and evaluator.ground_truth:
                                    # Ground Truthì—ì„œ í•´ë‹¹ ì¿¼ë¦¬ì™€ ë ˆìŠ¤í† ë‘ IDë¡œ ê´€ë ¨ ë¬¸ì„œ ì°¾ê¸°
                                    queries = evaluator.ground_truth.get("queries", [])
                                    relevant_ids = set()
                                    for query_data in queries:
                                        if (query_data.get("query") == query_text or 
                                            (restaurant_id and query_data.get("restaurant_id") == restaurant_id)):
                                            relevant_ids.update(query_data.get("relevant_review_ids", []))
                                    
                                    if relevant_ids:
                                        # Precision@k, Recall@k ê³„ì‚°
                                        precision_at_k = {}
                                        recall_at_k = {}
                                        for k in k_values:
                                            precision_at_k[f"P@{k}"] = evaluator.calculate_precision_at_k(
                                                retrieved_ids=retrieved_review_ids,
                                                relevant_ids=relevant_ids,
                                                k=k
                                            )
                                            recall_at_k[f"R@{k}"] = evaluator.calculate_recall_at_k(
                                                retrieved_ids=retrieved_review_ids,
                                                relevant_ids=relevant_ids,
                                                k=k
                                            )
                                        
                                        if precision_at_k or recall_at_k:
                                            print_info("Precision@k / Recall@k í‰ê°€ (ì„ë² ë”© ëª¨ë¸ ì •í™•ë„):")
                                            for k in k_values:
                                                k_key_p = f"P@{k}"
                                                k_key_r = f"R@{k}"
                                                precision = precision_at_k.get(k_key_p, 0.0)
                                                recall = recall_at_k.get(k_key_r, 0.0)
                                                if isinstance(precision, (int, float)):
                                                    print(f"  - {k_key_p}: {float(precision):.4f} ({float(precision)*100:.2f}%)")
                                                if isinstance(recall, (int, float)):
                                                    print(f"  - {k_key_r}: {float(recall):.4f} ({float(recall)*100:.2f}%)")
                                        
                                        precision_metrics = {
                                            "k_values": k_values,
                                            "precision_at_k": precision_at_k,
                                            "recall_at_k": recall_at_k,
                                            "total_queries": 1,
                                            "evaluated_queries": 1 if relevant_ids else 0,
                                        }
                        else:
                            print_warning(f"Ground Truth íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {ground_truth_path}")
                    except Exception as e:
                        print_warning(f"Precision@k / Recall@k í‰ê°€ ì‹¤íŒ¨: {str(e)}")
                
                # JSON ì €ì¥ìš© ë©”íŠ¸ë¦­ ìˆ˜ì§‘
                test_metrics["ë¦¬ë·° ì´ë¯¸ì§€ ê²€ìƒ‰"] = {
                    "performance": {
                        "avg_latency_sec": stats.get("avg_latency_sec"),
                        "min_latency_sec": stats.get("min_latency_sec"),
                        "max_latency_sec": stats.get("max_latency_sec"),
                        "p95_latency_sec": stats.get("p95_latency_sec"),
                        "p99_latency_sec": stats.get("p99_latency_sec"),
                        "success_rate": stats.get("success_rate"),
                        "success_count": stats.get("success_count"),
                        "total_iterations": stats.get("total_iterations"),
                        "throughput_req_per_sec": stats.get("throughput_req_per_sec"),
                        "target_avg_sec": target_avg,
                        "target_p95_sec": target_p95,
                        "target_p99_sec": target_p99,
                        "target_avg_achieved": avg_time <= target_avg,
                        "target_p95_achieved": p95_time <= target_p95,
                        "target_p99_achieved": p99_time <= target_p99,
                    },
                    "sqlite_metrics": db_metrics if db_metrics else None,
                    "accuracy": precision_metrics if precision_metrics else None,  # Precision@k / Recall@k ë©”íŠ¸ë¦­
                }
                
                return True
            else:
                print_error("ì„±ëŠ¥ ì¸¡ì • ì‹¤íŒ¨")
                return False
        else:
            # ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ
            start_time = time.time()
            response = requests.post(url, json=payload, timeout=60)
            elapsed_time = time.time() - start_time
            
            if response.status_code == 200:
                data = response.json()
                print_success(f"ë¦¬ë·° ì´ë¯¸ì§€ ê²€ìƒ‰ ì„±ê³µ (ì†Œìš” ì‹œê°„: {elapsed_time:.2f}ì´ˆ)")
                print(f"  - ê²€ìƒ‰ ê²°ê³¼ ìˆ˜: {len(data.get('results', []))}")
                print(f"  - ì´ ê²°ê³¼ ìˆ˜: {data.get('total', 0)}")
                
                # ìƒìœ„ 3ê°œ ê²°ê³¼ ì¶œë ¥
                for i, result in enumerate(data.get('results', [])[:3], 1):
                    print(f"  ê²°ê³¼ {i}:")
                    print(f"    - ë ˆìŠ¤í† ë‘ ID: {result.get('restaurant_id', 'N/A')}")
                    print(f"    - ë¦¬ë·° ID: {result.get('review_id', 'N/A')}")
                    print(f"    - ì´ë¯¸ì§€ URL: {result.get('image_url', 'N/A')[:50]}...")
                    review = result.get('review', {})
                    content = review.get('content', 'N/A')
                    if isinstance(content, str) and len(content) > 50:
                        print(f"    - ë¦¬ë·° ë‚´ìš©: {content[:50]}...")
                    else:
                        print(f"    - ë¦¬ë·° ë‚´ìš©: {content}")
                
                # JSON ì €ì¥ìš© ë©”íŠ¸ë¦­ ìˆ˜ì§‘
                test_metrics["ë¦¬ë·° ì´ë¯¸ì§€ ê²€ìƒ‰"] = {
                    "performance": {
                        "elapsed_time_sec": elapsed_time,
                        "result_count": len(data.get('results', [])),
                        "total_count": data.get('total', 0),
                    },
                    "sqlite_metrics": None,
                    "accuracy": None,
                }
                
                return True
            else:
                print_error(f"ë¦¬ë·° ì´ë¯¸ì§€ ê²€ìƒ‰ ì‹¤íŒ¨: {response.status_code}")
                print(f"  ì‘ë‹µ: {response.text[:200]}")
                return False
    except Exception as e:
        print_error(f"ë¦¬ë·° ì´ë¯¸ì§€ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return False


def run_tests_for_model(
    model_name: str,
    provider: str,
    enable_benchmark: bool = False,
    iterations: int = 5,
    tests: Optional[List[str]] = None,
) -> Dict[str, Any]:
    """
    íŠ¹ì • ëª¨ë¸ì— ëŒ€í•œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    
    Args:
        model_name: í…ŒìŠ¤íŠ¸í•  ëª¨ë¸ëª…
        provider: LLM ì œê³µì ("openai", "local", "runpod")
        enable_benchmark: ì„±ëŠ¥ ì¸¡ì • ëª¨ë“œ í™œì„±í™” ì—¬ë¶€
        iterations: ì„±ëŠ¥ ì¸¡ì • ë°˜ë³µ íšŸìˆ˜
        
    Returns:
        í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
    original_provider = os.getenv("LLM_PROVIDER")
    original_model = os.getenv("OPENAI_MODEL") if provider == "openai" else os.getenv("LLM_MODEL")
    
    try:
        os.environ["LLM_PROVIDER"] = provider
        if provider == "openai":
            os.environ["OPENAI_MODEL"] = model_name
        else:
            os.environ["LLM_MODEL"] = model_name
        
        print_header(f"ëª¨ë¸ í…ŒìŠ¤íŠ¸: {model_name} ({provider})")
        print_info(f"ì„œë²„ URL: {BASE_URL}")
        
        # test_metrics ì´ˆê¸°í™” (ëª¨ë¸ë³„ë¡œ ë…ë¦½ì ìœ¼ë¡œ ê´€ë¦¬)
        global test_metrics
        original_test_metrics = test_metrics.copy()
        test_metrics.clear()
        
        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        selected_tests = tests or ["summarize", "summarize_batch"]
        if "all" in selected_tests:
            selected_tests = ["sentiment", "sentiment_batch", "summarize", "summarize_batch", "strength", "vector", "image_search"]

        test_registry = {
            "sentiment": ("ê°ì„± ë¶„ì„", lambda: test_sentiment_analysis(enable_benchmark=enable_benchmark, num_iterations=iterations)),
            "sentiment_batch": ("ë°°ì¹˜ ê°ì„± ë¶„ì„", lambda: test_sentiment_analysis_batch(enable_benchmark=enable_benchmark, num_iterations=iterations)),
            "summarize": ("ë¦¬ë·° ìš”ì•½", lambda: test_summarize(enable_benchmark=enable_benchmark, num_iterations=iterations)),
            "summarize_batch": ("ë°°ì¹˜ ë¦¬ë·° ìš”ì•½", lambda: test_summarize_batch(enable_benchmark=enable_benchmark, num_iterations=iterations)),
            "strength": ("ê°•ì  ì¶”ì¶œ", lambda: test_extract_strengths(enable_benchmark=enable_benchmark, num_iterations=iterations)),
            "vector": ("ë²¡í„° ê²€ìƒ‰", lambda: test_vector_search(enable_benchmark=enable_benchmark, num_iterations=iterations)),
            "image_search": ("ë¦¬ë·° ì´ë¯¸ì§€ ê²€ìƒ‰", lambda: test_review_image_search(enable_benchmark=enable_benchmark, num_iterations=iterations)),
        }

        results = []
        for key in selected_tests:
            if key not in test_registry:
                print_warning(f"ì•Œ ìˆ˜ ì—†ëŠ” í…ŒìŠ¤íŠ¸ í•­ëª©: {key} (skip)")
                continue
            label, fn = test_registry[key]
            results.append((label, fn()))
        
        # test_metrics ì €ì¥ (ëª¨ë¸ë³„ë¡œ)
        model_test_metrics = test_metrics.copy()
        
        # resultsë¥¼ qwen.jsonê³¼ ìœ ì‚¬í•œ êµ¬ì¡°ë¡œë„ ì œê³µ (í˜•ì‹ ë³€í™˜ X, ë°˜í™˜ê°’ì—ë§Œ í¬í•¨)
        # - compare_models ì €ì¥ ì‹œ ì´ êµ¬ì¡°ë¥¼ ê·¸ëŒ€ë¡œ ë¤í”„í•˜ë©´ ëª¨ë“  ë©”íŠ¸ë¦­ì´ í¬í•¨ë¨
        test_results: Dict[str, Any] = {}
        for test_name, ok in results:
            test_result_dict: Dict[str, Any] = {
                "status": "passed" if ok else "failed",
                "success": ok,
            }
            if test_name in model_test_metrics:
                # performance/sqlite_metrics/accuracy ë“± ëª¨ë“  ë©”íŠ¸ë¦­ í¬í•¨
                test_result_dict.update(model_test_metrics[test_name])
            test_results[test_name] = test_result_dict
        
        # ê²°ê³¼ ì§‘ê³„
        success_count = sum(1 for _, result in results if result)
        total_count = len(results)
        
        # test_metrics ë³µì›
        test_metrics.clear()
        test_metrics.update(original_test_metrics)
        
        return {
            "model_name": model_name,
            "provider": provider,
            "success_count": success_count,
            "total_count": total_count,
            "success_rate": (success_count / total_count * 100) if total_count > 0 else 0,
            "results": results,  # ê¸°ì¡´ í˜¸í™˜
            "test_results": test_results,  # ê¶Œì¥: í…ŒìŠ¤íŠ¸ë³„ + ë©”íŠ¸ë¦­ê¹Œì§€ í¬í•¨ëœ êµ¬ì¡°
            "test_metrics": model_test_metrics,  # ëª¨ë“  ë©”íŠ¸ë¦­ ì›ë³¸ (ë””ë²„ê¹…/í›„ì²˜ë¦¬ìš©)
        }
    finally:
        # í™˜ê²½ ë³€ìˆ˜ ë³µì›
        if original_provider:
            os.environ["LLM_PROVIDER"] = original_provider
        else:
            os.environ.pop("LLM_PROVIDER", None)
        
        if provider == "openai":
            if original_model:
                os.environ["OPENAI_MODEL"] = original_model
            else:
                os.environ.pop("OPENAI_MODEL", None)
        else:
            if original_model:
                os.environ["LLM_MODEL"] = original_model
            else:
                os.environ.pop("LLM_MODEL", None)


def compare_models(
    models: List[str],
    provider: str,
    enable_benchmark: bool = False,
    iterations: int = 5,
    save_results: Optional[str] = None,
    generate_report: bool = False,
    tests: Optional[List[str]] = None,
    base_ports: Optional[List[int]] = None,
    test_data: Optional[Dict[str, Any]] = None,
) -> Dict[str, Any]:
    """
    ì—¬ëŸ¬ ëª¨ë¸ ë¹„êµ í…ŒìŠ¤íŠ¸
    
    Args:
        models: ë¹„êµí•  ëª¨ë¸ëª… ë¦¬ìŠ¤íŠ¸
        provider: LLM ì œê³µì ("openai", "local", "runpod")
        enable_benchmark: ì„±ëŠ¥ ì¸¡ì • ëª¨ë“œ í™œì„±í™” ì—¬ë¶€
        iterations: ì„±ëŠ¥ ì¸¡ì • ë°˜ë³µ íšŸìˆ˜
        save_results: ê²°ê³¼ë¥¼ ì €ì¥í•  JSON íŒŒì¼ ê²½ë¡œ
        generate_report: ë¹„êµ ë¦¬í¬íŠ¸ ìƒì„± ì—¬ë¶€
        base_ports: ê° ëª¨ë¸ë³„ ì„œë²„ í¬íŠ¸ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ìë™ í• ë‹¹: 8001ë¶€í„° ì‹œì‘)
        test_data: ì—…ë¡œë“œí•  í…ŒìŠ¤íŠ¸ ë°ì´í„° (ê° í¬íŠ¸ë³„ë¡œ ì—…ë¡œë“œ)
        
    Returns:
        ë¹„êµ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    global BASE_URL
    
    # í¬íŠ¸ ìë™ í• ë‹¹ (ì§€ì •ë˜ì§€ ì•Šì€ ê²½ìš°)
    if base_ports is None:
        base_ports = [8001 + i for i in range(len(models))]
    
    if len(base_ports) != len(models):
        print_error(f"í¬íŠ¸ ê°œìˆ˜({len(base_ports)})ì™€ ëª¨ë¸ ê°œìˆ˜({len(models)})ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        sys.exit(1)
    
    print_header(f"ì—¬ëŸ¬ ëª¨ë¸ ë¹„êµ í…ŒìŠ¤íŠ¸ ({len(models)}ê°œ ëª¨ë¸)")
    print_info(f"ì œê³µì: {provider}")
    print_info(f"ëª¨ë¸ ëª©ë¡: {', '.join(models)}")
    print_info("\nê° ëª¨ë¸ì€ ë³„ë„ í¬íŠ¸ì—ì„œ ì‹¤í–‰ ì¤‘ì´ì–´ì•¼ í•©ë‹ˆë‹¤:")
    for model, port in zip(models, base_ports):
        print_info(f"  - {model}: http://localhost:{port}")
    
    all_results = {}
    original_base_url = BASE_URL
    
    for i, (model_name, port) in enumerate(zip(models, base_ports), 1):
        print(f"\n{'='*60}")
        print(f"ëª¨ë¸ {i}/{len(models)}: {model_name} (í¬íŠ¸: {port})")
        print(f"{'='*60}\n")
        
        # BASE_URLì„ í•´ë‹¹ ëª¨ë¸ì˜ í¬íŠ¸ë¡œ ì„ì‹œ ë³€ê²½
        BASE_URL = f"http://localhost:{port}"
        
        try:
            # ì„œë²„ ì—°ê²° í™•ì¸
            try:
                response = requests.get(f"{BASE_URL}/health", timeout=5)
                if response.status_code != 200:
                    print_warning(f"í¬íŠ¸ {port}ì˜ ì„œë²„ê°€ ì‘ë‹µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. (ìƒíƒœ ì½”ë“œ: {response.status_code})")
            except Exception as e:
                print_error(f"í¬íŠ¸ {port}ì˜ ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
                print_info(f"ë‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ ì„œë²„ë¥¼ ì‹œì‘í•˜ì„¸ìš”:")
                if provider == "openai":
                    print_info(f"  LLM_PROVIDER={provider} OPENAI_MODEL={model_name} uvicorn app:app --port {port}")
                else:
                    print_info(f"  LLM_PROVIDER={provider} LLM_MODEL={model_name} uvicorn app:app --port {port}")
                all_results[model_name] = {
                    "model_name": model_name,
                    "provider": provider,
                    "success_count": 0,
                    "total_count": 0,
                    "success_rate": 0,
                    "results": [],
                    "error": f"ì„œë²„ ì—°ê²° ì‹¤íŒ¨ (í¬íŠ¸ {port})"
                }
                continue
            
            # ê° í¬íŠ¸ë³„ë¡œ ë°ì´í„° ì—…ë¡œë“œ
            if test_data:
                print_info(f"í¬íŠ¸ {port}ì— í…ŒìŠ¤íŠ¸ ë°ì´í„° ì—…ë¡œë“œ ì¤‘...")
                if upload_data_to_qdrant(test_data):
                    print_success(f"í¬íŠ¸ {port}ì— ë°ì´í„° ì—…ë¡œë“œ ì™„ë£Œ")
                else:
                    print_warning(f"í¬íŠ¸ {port}ì— ë°ì´í„° ì—…ë¡œë“œ ì‹¤íŒ¨. í…ŒìŠ¤íŠ¸ê°€ ì‹¤íŒ¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            
            result = run_tests_for_model(
                model_name=model_name,
                provider=provider,
                enable_benchmark=enable_benchmark,
                iterations=iterations,
                tests=tests,
            )
            all_results[model_name] = result
        finally:
            # BASE_URL ë³µì›
            BASE_URL = original_base_url
        
        # ëª¨ë¸ ê°„ ê°„ê²©
        if i < len(models):
            print_info("ë‹¤ìŒ ëª¨ë¸ í…ŒìŠ¤íŠ¸ë¡œ ì´ë™...")
            time.sleep(2)  # ì§§ì€ ëŒ€ê¸°
    
    # ë¹„êµ ë¦¬í¬íŠ¸ ìƒì„±
    if generate_report:
        print_header("ëª¨ë¸ ë¹„êµ ë¦¬í¬íŠ¸")
        print("\nì„±ê³µë¥  ë¹„êµ:")
        for model_name, result in all_results.items():
            success_rate = result.get("success_rate", 0)
            status = "âœ“" if success_rate == 100 else "âš " if success_rate >= 50 else "âœ—"
            print(f"  {status} {model_name}: {success_rate:.1f}% ({result['success_count']}/{result['total_count']})")
    
    # ê²°ê³¼ ì €ì¥ (í˜•ì‹ ë³€í™˜ ì—†ì´, ë°˜í™˜ê°’ ê·¸ëŒ€ë¡œ ì €ì¥)
    if save_results:
        with open(save_results, 'w', encoding='utf-8') as f:
            json.dump(all_results, f, ensure_ascii=False, indent=2)
        print_success(f"ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {save_results}")
    
    return all_results


def main():
    """
    ë©”ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    
    ì—¬ëŸ¬ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì§€ì›:
    - ë‹¨ì¼ ëª¨ë¸: --model ì˜µì…˜ ì‚¬ìš©
    - ì—¬ëŸ¬ ëª¨ë¸ ë¹„êµ: --compare-models ì˜µì…˜ ì‚¬ìš©
    - í™˜ê²½ ë³€ìˆ˜ ê¸°ë°˜: í™˜ê²½ ë³€ìˆ˜ë§Œ ì„¤ì •í•˜ì—¬ ì‹¤í–‰
    """
    parser = argparse.ArgumentParser(
        description="ì „ì²´ ê¸°ëŠ¥ í†µí•© í…ŒìŠ¤íŠ¸ (ë‹¤ì¤‘ ëª¨ë¸ ì§€ì›)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì œ:
  # ë‹¨ì¼ ëª¨ë¸ í…ŒìŠ¤íŠ¸
  python test_openai_all.py --model "gpt-4o-mini" --provider openai
  
  # ì—¬ëŸ¬ ëª¨ë¸ ë¹„êµ
  python test_openai_all.py --compare-models \\
      --models "gpt-4o-mini" "gpt-3.5-turbo" \\
      --provider openai \\
      --benchmark \\
      --save-results results.json
  
  # ë¶€í•˜í…ŒìŠ¤íŠ¸ (baseline(ëŒ€í‘œ ì„±ëŠ¥) ì¸¡ì •)

  python test_openai_all.py --load-test \
      --total-requests 500 \
      --concurrent-users 5 \
      --ramp-up 20 \
      --save-results load_test_baseline_results.json
      
# ë¶€í•˜í…ŒìŠ¤íŠ¸ (stress(í•œê³„ í™•ì¸) ì¸¡ì •)

  python test_openai_all.py --load-test \
      --total-requests 1000 \
      --concurrent-users 15 \
      --ramp-up 30 \
      --save-results load_test_stress_results.json
  
  # í™˜ê²½ ë³€ìˆ˜ ê¸°ë°˜ (ê¸°ì¡´ ë°©ì‹)
  export LLM_PROVIDER="openai"
  export OPENAI_MODEL="gpt-4o-mini"
  python test_openai_all.py --benchmark
        """
    )
    parser.add_argument(
        "--benchmark",
        action="store_true",
        help="ì„±ëŠ¥ ì¸¡ì • ëª¨ë“œ í™œì„±í™” (QUANTITATIVE_METRICS.md ì§€í‘œ ì¸¡ì •)"
    )
    parser.add_argument(
        "--iterations",
        type=int,
        default=5,
        help="ì„±ëŠ¥ ì¸¡ì • ë°˜ë³µ íšŸìˆ˜ (ê¸°ë³¸ê°’: 5)"
    )
    parser.add_argument(
        "--model",
        type=str,
        help="í…ŒìŠ¤íŠ¸í•  ëª¨ë¸ëª… (ì˜ˆ: 'gpt-4o-mini', 'Qwen/Qwen2.5-7B-Instruct')"
    )
    parser.add_argument(
        "--provider",
        type=str,
        choices=["openai", "local", "runpod"],
        help="LLM ì œê³µì ì„ íƒ (openai, local, runpod)"
    )
    parser.add_argument(
        "--compare-models",
        action="store_true",
        help="ì—¬ëŸ¬ ëª¨ë¸ ë¹„êµ í…ŒìŠ¤íŠ¸ ëª¨ë“œ"
    )
    parser.add_argument(
        "--models",
        nargs="+",
        help="ë¹„êµí•  ëª¨ë¸ëª… ë¦¬ìŠ¤íŠ¸ (--compare-modelsì™€ í•¨ê»˜ ì‚¬ìš©)"
    )
    parser.add_argument(
        "--save-results",
        type=str,
        help="ê²°ê³¼ë¥¼ ì €ì¥í•  JSON íŒŒì¼ ê²½ë¡œ"
    )
    parser.add_argument(
        "--generate-report",
        action="store_true",
        help="ëª¨ë¸ ë¹„êµ ë¦¬í¬íŠ¸ ìƒì„± (--compare-modelsì™€ í•¨ê»˜ ì‚¬ìš©)"
    )
    parser.add_argument(
        "--load-test",
        action="store_true",
        help="ë¶€í•˜í…ŒìŠ¤íŠ¸ ëª¨ë“œ í™œì„±í™” (ë™ì‹œ ìš”ì²­ ì²˜ë¦¬ ëŠ¥ë ¥ ì¸¡ì •)"
    )
    parser.add_argument(
        "--tests",
        nargs="+",
        default=["all"],
        choices=["all", "sentiment", "sentiment_batch", "summarize", "summarize_batch", "strength", "vector", "image_search"],
        help="ì‹¤í–‰í•  í…ŒìŠ¤íŠ¸ ì„ íƒ (ê¸°ë³¸ê°’: summarize summarize_batch). ì˜ˆ: --tests summarize summarize_batch",
    )
    parser.add_argument(
        "--total-requests",
        type=int,
        default=100,
        help="ë¶€í•˜í…ŒìŠ¤íŠ¸ ì´ ìš”ì²­ ìˆ˜ (ê¸°ë³¸ê°’: 100)"
    )
    parser.add_argument(
        "--concurrent-users",
        type=int,
        default=10,
        help="ë¶€í•˜í…ŒìŠ¤íŠ¸ ë™ì‹œ ì‚¬ìš©ì ìˆ˜ (ê¸°ë³¸ê°’: 10)"
    )
    parser.add_argument(
        "--ramp-up",
        type=int,
        default=0,
        help="ë¶€í•˜í…ŒìŠ¤íŠ¸ ì ì§„ì  ë¶€í•˜ ì¦ê°€ ì‹œê°„(ì´ˆ) (ê¸°ë³¸ê°’: 0, ì¦‰ì‹œ ì‹œì‘)"
    )
    parser.add_argument(
        "--generate-from-kr3",
        action="store_true",
        help="kr3.tsvì—ì„œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± (ê¸°ë³¸ê°’: test_data_sample.json ì‚¬ìš©)"
    )
    parser.add_argument(
        "--kr3-sample",
        type=int,
        default=None,
        help="kr3.tsvì—ì„œ ìƒ˜í”Œë§í•  ë¦¬ë·° ìˆ˜ (--generate-from-kr3ì™€ í•¨ê»˜ ì‚¬ìš©)"
    )
    parser.add_argument(
        "--kr3-restaurants",
        type=int,
        default=None,
        help="ìƒì„±í•  ë ˆìŠ¤í† ë‘ ìˆ˜ (--generate-from-kr3ì™€ í•¨ê»˜ ì‚¬ìš©)"
    )
    parser.add_argument(
        "--ports",
        type=int,
        nargs="+",
        help="ê° ëª¨ë¸ë³„ ì„œë²„ í¬íŠ¸ ë¦¬ìŠ¤íŠ¸ (--compare-modelsì™€ í•¨ê»˜ ì‚¬ìš©). ì˜ˆ: --ports 8001 8002 8003. ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ 8001ë¶€í„° ìë™ í• ë‹¹"
    )
    args = parser.parse_args()
    
    print_header("RunPod Pod ì„œë²„ API ì „ì²´ ê¸°ëŠ¥ í†µí•© í…ŒìŠ¤íŠ¸")
    print_info(f"ì„œë²„ URL: {BASE_URL}")
    print_info("RunPod Podì—ì„œ ì‹¤í–‰ ì¤‘ì¸ FastAPI ì„œë²„ë¥¼ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤")
    
    if args.benchmark:
        print_info("ì„±ëŠ¥ ì¸¡ì • ëª¨ë“œ í™œì„±í™” (QUANTITATIVE_METRICS.md ì§€í‘œ ì¸¡ì •)")
        print_info(f"ë°˜ë³µ íšŸìˆ˜: {args.iterations}")
    
    # í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (--provider ì˜µì…˜ì´ ìˆìœ¼ë©´ ì ìš©)
    if args.provider:
        os.environ["LLM_PROVIDER"] = args.provider
        print_info(f"LLM_PROVIDER ì„¤ì •: {args.provider}")
    
    # í™˜ê²½ ë³€ìˆ˜ í™•ì¸
    llm_provider = os.getenv("LLM_PROVIDER", "")
    openai_key = os.getenv("OPENAI_API_KEY", "")
    
    # --model ì˜µì…˜ì´ ìˆìœ¼ë©´ í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
    if args.model:
        if args.provider == "openai" or (not args.provider and llm_provider == "openai"):
            os.environ["OPENAI_MODEL"] = args.model
            print_info(f"OPENAI_MODEL ì„¤ì •: {args.model}")
        else:
            os.environ["LLM_MODEL"] = args.model
            print_info(f"LLM_MODEL ì„¤ì •: {args.model}")
    
    if llm_provider == "local":
        llm_model = os.getenv("LLM_MODEL", "")
        if llm_model == "Qwen/Qwen2.5-7B-Instruct":
            print_info("Qwen/Qwen2.5-7B-Instruct ëª¨ë¸ ì‚¬ìš©")
        elif llm_model == "meta-llama/Llama-3.1-8B-Instruct":
            print_info("meta-llama/Llama-3.1-8B-Instruct ëª¨ë¸ ì‚¬ìš©")
        elif llm_model == "google/gemma-2-9b-it":
            print_info("google/gemma-2-9b-it ëª¨ë¸ ì‚¬ìš©")
        elif llm_model == "LGAI-EXAONE/K-EXAONE-236B-A23B-GGUF":
            print_info("LGAI-EXAONE/K-EXAONE-236B-A23B-GGUF ëª¨ë¸ ì‚¬ìš©")
        elif llm_model == "unsloth/DeepSeek-R1-GGUF":
            print_info("unsloth/DeepSeek-R1-GGUF ëª¨ë¸ ì‚¬ìš©")
        elif llm_model:
            print_info(f"ë¡œì»¬ ëª¨ë¸ ì‚¬ìš©: {llm_model}")
    
    # OpenAI ëª¨ë¸ í™•ì¸
    openai_model = os.getenv("OPENAI_MODEL", "")
    if openai_model:
        print_info(f"OpenAI ëª¨ë¸ ì‚¬ìš©: {openai_model}")
    
    if llm_provider and llm_provider != "openai":
        print_info(f"LLM_PROVIDER: {llm_provider}")
    
    if not openai_key:
        print_warning("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print_info("ë‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ ì„¤ì •í•˜ì„¸ìš”: export OPENAI_API_KEY='your_api_key'")
    # OpenAI API í‚¤ í™•ì¸ ë©”ì‹œì§€ ì œê±°
    
    # ì„œë²„ í—¬ìŠ¤ ì²´í¬
    if not check_server_health():
        sys.exit(1)
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
    data_result = generate_test_data(
        generate_from_kr3=args.generate_from_kr3,
        kr3_sample=args.kr3_sample,
        kr3_restaurants=args.kr3_restaurants,
    )
    temp_json_path = None
    test_data = None
    
    if data_result:
        data, temp_json_path = data_result
        test_data = data  # compare_modelsì— ì „ë‹¬í•  ë°ì´í„° ì €ì¥
        
        # SAMPLE_RESTAURANT_IDì™€ SAMPLE_REVIEWSë¥¼ ì‹¤ì œ ë°ì´í„°ë¡œ ì—…ë°ì´íŠ¸
        if data.get("restaurants"):
            global SAMPLE_RESTAURANT_ID, SAMPLE_REVIEWS
            first_restaurant = data["restaurants"][0]
            SAMPLE_RESTAURANT_ID = first_restaurant.get("restaurant_id", 1)
            SAMPLE_REVIEWS = first_restaurant.get("reviews", [])
            print_info(f"í…ŒìŠ¤íŠ¸ ë ˆìŠ¤í† ë‘ ID: {SAMPLE_RESTAURANT_ID}")
            print_info(f"í…ŒìŠ¤íŠ¸ ë¦¬ë·° ìˆ˜: {len(SAMPLE_REVIEWS)}ê°œ")
    
    # ëª¨ë¸ ë¹„êµ ëª¨ë“œ ì²˜ë¦¬
    if args.compare_models:
        if not args.models or not args.provider:
            print_error("--compare-models ëª¨ë“œì—ì„œëŠ” --modelsì™€ --provider ì˜µì…˜ì´ í•„ìš”í•©ë‹ˆë‹¤.")
            print_info("ì‚¬ìš© ì˜ˆ: python test_all_task.py --compare-models --models 'model1' 'model2' --provider openai --benchmark --save-results results.json")
            print_info("í¬íŠ¸ ì§€ì • ì˜ˆ: python test_all_task.py --compare-models --models 'model1' 'model2' --provider local --ports 8001 8002")
            sys.exit(1)
        
        # í¬íŠ¸ ê²€ì¦
        if args.ports and len(args.ports) != len(args.models):
            print_error(f"í¬íŠ¸ ê°œìˆ˜({len(args.ports)})ì™€ ëª¨ë¸ ê°œìˆ˜({len(args.models)})ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            sys.exit(1)
        
        # compare_models() í•¨ìˆ˜ í˜¸ì¶œ
        comparison_results = compare_models(
            models=args.models,
            provider=args.provider,
            enable_benchmark=args.benchmark,
            iterations=args.iterations,
            save_results=args.save_results,
            generate_report=args.generate_report,
            tests=args.tests,
            base_ports=args.ports,
            test_data=test_data,
        )
        
        # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        print_header("ëª¨ë¸ ë¹„êµ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        if args.save_results:
            print_success(f"ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {args.save_results}")
        
        # ì„ì‹œ íŒŒì¼ ì •ë¦¬
        if temp_json_path and os.path.exists(temp_json_path):
            try:
                os.unlink(temp_json_path)
            except Exception:
                pass
        
        sys.exit(0)
    
    # ì¼ë°˜ ëª¨ë“œ: ë°ì´í„° ì—…ë¡œë“œ (compare_models ëª¨ë“œê°€ ì•„ë‹ ë•Œë§Œ)
    if test_data:
        if upload_data_to_qdrant(test_data):
            print_success("í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ")
        else:
            print_warning("Qdrant upload ì‹¤íŒ¨. ì¼ë¶€ í…ŒìŠ¤íŠ¸ê°€ ì‹¤íŒ¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    else:
        print_warning("í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± ì‹¤íŒ¨. ì¼ë¶€ í…ŒìŠ¤íŠ¸ê°€ ì‹¤íŒ¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    
    # ì„ì‹œ íŒŒì¼ ì •ë¦¬
    if temp_json_path and os.path.exists(temp_json_path):
        try:
            os.unlink(temp_json_path)
        except Exception:
            pass
    
    # ë¶€í•˜í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì²˜ë¦¬
    if args.load_test:
        print_header("ë¶€í•˜í…ŒìŠ¤íŠ¸ ëª¨ë“œ")
        print_info(f"ì´ ìš”ì²­ ìˆ˜: {args.total_requests}")
        print_info(f"ë™ì‹œ ì‚¬ìš©ì ìˆ˜: {args.concurrent_users}")
        if args.ramp_up > 0:
            print_info(f"ì ì§„ì  ë¶€í•˜ ì¦ê°€: {args.ramp_up}ì´ˆ")
        
        # ê° ì—”ë“œí¬ì¸íŠ¸ì— ëŒ€í•´ ë¶€í•˜í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        load_test_results = {}
        
        # 1. ë°°ì¹˜ ê°ì„± ë¶„ì„ ë¶€í•˜í…ŒìŠ¤íŠ¸
        print_header("1. ë°°ì¹˜ ê°ì„± ë¶„ì„ ë¶€í•˜í…ŒìŠ¤íŠ¸")
        url = f"{BASE_URL}{API_PREFIX}/sentiment/analyze/batch"
        # 10ê°œ ë ˆìŠ¤í† ë‘ ë°°ì¹˜ ìƒì„±
        restaurants_payload = []
        for i in range(10):
            restaurants_payload.append({
                "restaurant_id": SAMPLE_RESTAURANT_ID + i,
                "reviews": SAMPLE_REVIEWS  # ëª¨ë“  ë ˆìŠ¤í† ë‘ì— ë™ì¼í•œ ë¦¬ë·° ì‚¬ìš©
            })
        payload = {
            "restaurants": restaurants_payload
        }
        success, stats = load_test(
            endpoint=url,
            payload=payload,
            total_requests=args.total_requests,
            concurrent_users=args.concurrent_users,
            timeout=120,
            ramp_up_seconds=args.ramp_up
        )
        if success and stats:
            print_success("ë°°ì¹˜ ê°ì„± ë¶„ì„ ë¶€í•˜í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
            print(f"  - í‰ê·  ì‘ë‹µ ì‹œê°„: {stats['avg_latency_sec']:.3f}ì´ˆ")
            print(f"  - P50 ì‘ë‹µ ì‹œê°„: {stats.get('p50_latency_sec', 'N/A'):.3f}ì´ˆ" if stats.get('p50_latency_sec') else "  - P50 ì‘ë‹µ ì‹œê°„: N/A")
            print(f"  - P95 ì‘ë‹µ ì‹œê°„: {stats['p95_latency_sec']:.3f}ì´ˆ")
            print(f"  - P99 ì‘ë‹µ ì‹œê°„: {stats['p99_latency_sec']:.3f}ì´ˆ")
            print(f"  - ì²˜ë¦¬ëŸ‰: {stats['throughput_req_per_sec']:.2f} req/s")
            print(f"  - ì„±ê³µë¥ : {stats['success_rate']:.1f}% ({stats['success_count']}/{stats['total_requests']})")
            print(f"  - ìµœëŒ€ ë™ì‹œ ìš”ì²­ ìˆ˜: {stats.get('max_concurrent_requests', 'N/A')}")
            load_test_results["ë°°ì¹˜ ê°ì„± ë¶„ì„"] = stats
        
        # 2. ë°°ì¹˜ ë¦¬ë·° ìš”ì•½ ë¶€í•˜í…ŒìŠ¤íŠ¸
        print_header("2. ë°°ì¹˜ ë¦¬ë·° ìš”ì•½ ë¶€í•˜í…ŒìŠ¤íŠ¸")
        url = f"{BASE_URL}{API_PREFIX}/llm/summarize/batch"
        payload = {
            "restaurants": [
                {
                    "restaurant_id": SAMPLE_RESTAURANT_ID,
                    "positive_query": "ë§›ìˆë‹¤ ì¢‹ë‹¤ ë§Œì¡±",
                    "negative_query": "ë§›ì—†ë‹¤ ë³„ë¡œ ë¶ˆë§Œ",
                    "limit": 10,
                    "min_score": 0.0
                },
                {
                    "restaurant_id": SAMPLE_RESTAURANT_ID + 1,
                    "positive_query": "ë§›ìˆë‹¤ ì¢‹ë‹¤ ë§Œì¡±",
                    "negative_query": "ë§›ì—†ë‹¤ ë³„ë¡œ ë¶ˆë§Œ",
                    "limit": 10,
                    "min_score": 0.0
                }
            ]
        }
        success, stats = load_test(
            endpoint=url,
            payload=payload,
            total_requests=args.total_requests,
            concurrent_users=args.concurrent_users,
            timeout=180,
            ramp_up_seconds=args.ramp_up
        )
        if success and stats:
            print_success("ë°°ì¹˜ ë¦¬ë·° ìš”ì•½ ë¶€í•˜í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
            print(f"  - í‰ê·  ì‘ë‹µ ì‹œê°„: {stats['avg_latency_sec']:.3f}ì´ˆ")
            print(f"  - P50 ì‘ë‹µ ì‹œê°„: {stats.get('p50_latency_sec', 'N/A'):.3f}ì´ˆ" if stats.get('p50_latency_sec') else "  - P50 ì‘ë‹µ ì‹œê°„: N/A")
            print(f"  - P95 ì‘ë‹µ ì‹œê°„: {stats['p95_latency_sec']:.3f}ì´ˆ")
            print(f"  - P99 ì‘ë‹µ ì‹œê°„: {stats['p99_latency_sec']:.3f}ì´ˆ")
            print(f"  - ì²˜ë¦¬ëŸ‰: {stats['throughput_req_per_sec']:.2f} req/s")
            print(f"  - ì„±ê³µë¥ : {stats['success_rate']:.1f}% ({stats['success_count']}/{stats['total_requests']})")
            print(f"  - ìµœëŒ€ ë™ì‹œ ìš”ì²­ ìˆ˜: {stats.get('max_concurrent_requests', 'N/A')}")
            load_test_results["ë°°ì¹˜ ë¦¬ë·° ìš”ì•½"] = stats
        
        # ê²°ê³¼ ì €ì¥
        if args.save_results:
            load_test_output = {
                "timestamp": datetime.now().isoformat(),
                "server_url": BASE_URL,
                "load_test_mode": True,
                "total_requests": args.total_requests,
                "concurrent_users": args.concurrent_users,
                "ramp_up_seconds": args.ramp_up,
                "test_results": load_test_results,
            }
            with open(args.save_results, 'w', encoding='utf-8') as f:
                json.dump(load_test_output, f, ensure_ascii=False, indent=2)
            print_success(f"\në¶€í•˜í…ŒìŠ¤íŠ¸ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {args.save_results}")
        
        sys.exit(0)
    
    # ë‹¨ì¼ ëª¨ë¸ í…ŒìŠ¤íŠ¸ (ê¸°ì¡´ ë¡œì§)
    results = []
    results_dict = {}  # JSON ì €ì¥ìš©
    test_metrics.clear()  # í…ŒìŠ¤íŠ¸ ë©”íŠ¸ë¦­ ì´ˆê¸°í™”
    
    selected_tests = args.tests or ["summarize", "summarize_batch"]
    if "all" in selected_tests:
        selected_tests = ["sentiment", "sentiment_batch", "summarize", "summarize_batch", "strength", "vector", "image_search"]

    test_registry = {
        "sentiment": ("ê°ì„± ë¶„ì„", lambda: test_sentiment_analysis(enable_benchmark=args.benchmark, num_iterations=args.iterations)),
        "sentiment_batch": ("ë°°ì¹˜ ê°ì„± ë¶„ì„", lambda: test_sentiment_analysis_batch(enable_benchmark=args.benchmark, num_iterations=args.iterations)),
        "summarize": ("ë¦¬ë·° ìš”ì•½", lambda: test_summarize(enable_benchmark=args.benchmark, num_iterations=args.iterations)),
        "summarize_batch": ("ë°°ì¹˜ ë¦¬ë·° ìš”ì•½", lambda: test_summarize_batch(enable_benchmark=args.benchmark, num_iterations=args.iterations)),
        "strength": ("ê°•ì  ì¶”ì¶œ", lambda: test_extract_strengths(enable_benchmark=args.benchmark, num_iterations=args.iterations)),
        "vector": ("ë²¡í„° ê²€ìƒ‰", lambda: test_vector_search(enable_benchmark=args.benchmark, num_iterations=args.iterations)),
        "image_search": ("ë¦¬ë·° ì´ë¯¸ì§€ ê²€ìƒ‰", lambda: test_review_image_search(enable_benchmark=args.benchmark, num_iterations=args.iterations)),
    }

    for key in selected_tests:
        if key not in test_registry:
            print_warning(f"ì•Œ ìˆ˜ ì—†ëŠ” í…ŒìŠ¤íŠ¸ í•­ëª©: {key} (skip)")
            continue
        label, fn = test_registry[key]
        results.append((label, fn()))
    
    # ê²°ê³¼ ìš”ì•½
    print_header("í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    
    passed = sum(1 for _, result in results if result)
    total = len(results)
    
    # JSON ì €ì¥ìš© ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ êµ¬ì„±
    if args.save_results:
        results_dict = {
            "timestamp": datetime.now().isoformat(),
            "server_url": BASE_URL,
            "benchmark_mode": args.benchmark,
            "iterations": args.iterations if args.benchmark else None,
            "model_info": {
                "llm_provider": "local",  # ê²°ê³¼ ì €ì¥ ì‹œ í•­ìƒ localë¡œ ì„¤ì •
                "llm_model": os.getenv("LLM_MODEL", os.getenv("OPENAI_MODEL", "")),
                "embedding_model": os.getenv("EMBEDDING_MODEL", ""),
                "sentiment_model": os.getenv("SENTIMENT_MODEL", ""),
            },
            "test_results": {},
            "summary": {
                "total_tests": total,
                "passed_tests": passed,
                "failed_tests": total - passed,
                "success_rate": (passed / total * 100) if total > 0 else 0,
            }
        }
        
        # ê° í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì¶”ê°€
        for name, result in results:
            test_result = {
                "status": "passed" if result else "failed",
                "success": result
            }
            # test_metricsì—ì„œ í•´ë‹¹ í…ŒìŠ¤íŠ¸ì˜ ì„±ëŠ¥/ì •í™•ë„ ë©”íŠ¸ë¦­ ì¶”ê°€
            if name in test_metrics:
                test_result.update(test_metrics[name])
            results_dict["test_results"][name] = test_result
    
    for name, result in results:
        if result:
            print_success(f"{name}: í†µê³¼")
        else:
            print_error(f"{name}: ì‹¤íŒ¨")
    
    print(f"\n{Colors.BOLD}ì´ {passed}/{total} í…ŒìŠ¤íŠ¸ í†µê³¼{Colors.RESET}")
    
    if args.benchmark:
        print_info("\nì„±ëŠ¥ ì¸¡ì • ëª¨ë“œë¡œ ì‹¤í–‰ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print_info("ë” ìì„¸í•œ ë©”íŠ¸ë¦­ì€ SQLite ë°ì´í„°ë² ì´ìŠ¤ë¥¼ í™•ì¸í•˜ì„¸ìš”:")
        print_info(f"  sqlite3 {METRICS_DB_PATH}")
        print_info("\nQUANTITATIVE_METRICS.mdì˜ SQL ì¿¼ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¶”ê°€ ë¶„ì„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
    
    # ê²°ê³¼ ì €ì¥
    if args.save_results and results_dict:
        with open(args.save_results, 'w', encoding='utf-8') as f:
            json.dump(results_dict, f, ensure_ascii=False, indent=2)
        print_success(f"\nê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {args.save_results}")
    
    if passed == total:
        print_success("ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!")
        sys.exit(0)
    else:
        print_error(f"{total - passed}ê°œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
        sys.exit(1)


if __name__ == "__main__":
    main()
