## 개선 사항

기존 방식

RUNPOD vllm 서버리스 엔드포인트에서 llm을 호출하는 방식

이 방식을 택했던 근거:

일반 GPU서버를 띄웠을 경우에 생기는 문제점
1. 우리 서비스가 상시 GPU가 필요한 서비스가 아님. 따라서 불필요한 비용 문제가 발생. 따라서 추론 요청시에만 GPU서버가 뜨고 죽는 방식인 서버리스 방식을 택했음.

서버리스 방식의 문제점
1. RUNPOD vllm 서버리스 엔드포인트 방식은 내가 vllm에 대한 커스터마이징이 불가했음. 따라서 내가 직접 vllm을 커스터마이징 하기 위해선 로컬 vllm이 필요했음. 이건 당장 커스터마이징 수요가 없더라도, 조작 자유도는 최대한 확보하는게 다양한 상황에 대응 가능하기에 최대한 확보하는게 중요하다고 생각함. 즉, 확장성, 유연성에 대한 이야기임.

해결방안에 대한 생각
1. 찾아보니 vllm은 GPU가 상주하는 서버에서만 있을 수 있다고 한다. 그러니까 사실은 서버리스 vllm이란 존재할 수 없다. 그렇다면 RUNPOD vllm endpoint란 무엇인가? 이것은 알고보니 진짜 '서버리스'가 아니고 '서버리스 같은' 것이다. 정확히는, vllm이 띄워진 GPU 서버가 있는 컨테이너가 뜨고 요청이 종료되면 그 컨테이너가 내려가는 방식이다. 따라서 마치 서버리스 처럼, 요청이 들어올때만 서버가 가동되는 것 처럼 보이는 것이다.

그래서 생각했다. 이 시스템이 진짜 서버리스가 아니고 컨테이너를 올렸다 내리는 방식이라면, 이것을 내가 직접 구현한다면 어떨까? 하고 말이다.

따라서, RUNPOD vllm 서버리스 엔드포인트 방식을 수동으로 직접 구현 하기로 마음 먹음.

구현 방안

1. 일반 서버인 RUNPOD의 POD GPU서버에 vllm 서빙을 한다.
2. fastapi를 통해 추론 요청을 vllm서버에 보내고, 응답을 받는다.
3. watchdog을 통해 GPU서버로부터 응답을 받고, vllm 프로세스가 종료되면 POD 컨테이너를 종료시킨다.
4. 이 모든 로그를 nvidia-smi를 통해 로그를 관리하여 모니터링.
