파이프라인
1차

1. sentiment
    kobert로 1차 분류 수행.
    따라서, llm이 분류 수행해 긍/부정 반환. "음식이 맛있는데 직원이 친절하다" 는 중립리뷰지만, positive, negative 둘중에 하나로 집계할 수가 없고, 감정이 두개가 나타났으므로 감성분석의 목적을 위해서는 모든 감정을 집계해야 한다고 생각함.  따라서 해당 문장들은 llm이 ['는데', '지만']을 기준으로 긍/부정 각각을 집계하게 만듬.

    최종적으로 positive_count, negative_count, positive_ratio, negative_ratio
2. summary
    벡터 검색을 이용해 긍정 리뷰와 부정 리뷰 각각을 취합하고, 그걸 llm에게 context로 주고 두 리뷰를 활용해 총 요약을 수행.
3. strength
    강점 추출 대상 음식점에 대해 벡터 검색으로 긍정적인 점 추출, 비교 음식점들 각각에 벡터 검색을 수행해 각 음식점의 긍정적인 점을 추출.
    llm이 이들을 context로 받아 강점 추출 대상 음식점이 다른 음식점들에 비해 어떤 점이 강점인지 말한다.

4. image 검색
    전체 음식점에 대해 ["분위기 좋다"](예시) 쿼리로 (dense 검색)벡터 검색을 진행해 해당 리뷰에 있는 image를 반환한다.

1차의 한계
1. sentiment
    kobert가 파인튜닝 된거지만 정확한 성능을 보이지는 않았음. "맛있지만 직원이 불친절하다" 같은 두개의 감정이 섞인 문장의 경우 중립 score를 반환하지 않아 이들만 LLM이 분류하는 전략을 사용했지만, ["지만","는데"]를 분리 기준으로 세웠지만, 2개의 감정은 다른 방식으로도 나타날 수 있으나 그거에 대해 전부 수동으로 추가를 하는 건 현실적으로 불가능함.

    또한, 모델 2개를 사용한다는 건, 디버깅도 번거롭고, llm이 어떤 문장에 어떤 부분을 긍/부정으로 파악했는지 추적하기도 어려움. 파이프라인의 복잡도가 높은데, 복잡도에 비해 그정도의 성능을 보이지는 않음.

2차 
1. sentiment
llm을 배치 단위로 호출해 각 문장을 보고 긍/부정 개수를 세서 총합을 반환하게 요청해 최대한 호출 횟수를 줄여봄.

3차의 sentiment로의 로직 변경 이유

vllm으로 서빙을 진행하면, continous batch 방식을 통해 훨씬 빠른 속도로 프로세스의 처리가 가능해지고, 이는 처리의 종류가 무엇이든 병렬 처리를 가능하게 함.(음식점 단위 task도 전체 음식점 대상으로 처리가 가능함.)

2. summary
위와 동일

3. strength
위와 동일

4. image 검색
위와 동일

3차
vllm 사용의 이유
우리 서비스는 전체 음식점에 대해 병렬 처리가 필요함. 또한 배치 단위로 처리가 필요함. vllm의 Continous batch는 배치를 통해 이런 처리가 빠르게 가능해짐.
또한 vllm에게 비동기 큐로 리뷰를 전달함. 비동기 큐 방식인 이유는 OOM(Out of memory)를 피하기 위함.
--> 비동기 큐를 통해 한개씩 리뷰를 전달해 GPU 메모리에 1개씩 보내 OOM 방지가 가능하다고 생각했음. 하지만, GPU는 비동기 큐와 관계 없이 입력 토큰 시퀀스의 길이,kv cache로 생성이 불가한 토큰 생성에 영향을 받음.

GPU 메모리 생성 방식
1. GPU에 입력
2. prefill로 한번에 인코딩, 인코딩한 만큼 kv cache 채우기
3. 디코딩이 일어나면서 kv cache로 재사용 가능하다 -> 메모리 증가 x
   kv cache에 저장된 기존의 어텐션으로 토큰 생성이 불가능하다 ->  그 토큰 만큼 메모리 증가 o
4. 

1. sentiment
    llm을 vllm으로 서빙, llm이 vllm에 의해 전체 리뷰를 continous batch 방식으로 처리를 진행함. 또한, 전체 음식점에 대해 비동기큐 방식으로 vllm에게 전달하여 처리를 진행함. 즉, 모든 음식점이 병렬로 감성 비율 계산이 이루어짐.
2. summary
    sentiment와 동일하게 음식점을 비동기큐 방식으로 넣고, continous batch를 사용해 태스크 수행. 즉, 모든 음식점이 병렬로 요약이 이루어짐.
3. strength
    이것도 비동기큐,continous batch를 이용해 모든 음식점들이 병렬로 강점이 추출됨.
4. image 검색
    1차와 동일함.

4차



summary 로직 변경

기존

벡터 db에서 긍정 3개 검색, 부정 3개 검색 -> llm이 긍정 요약, 부정 요약 수행 -> llm이 긍정/부정 총 요약 수행 (llm 총 3개의 task 수행)

현재

벡터 db에서 긍정 3개 검색, 부정 3개 검색 -> llm이 긍정 3개, 부정 3개를 가지고 총 요약 수행 (llm 총 1개 task 수행)

변경 이유

기존에 llm이 3개의 태스크를 수행하는게 불필요하다고 판단. 즉, 불필요한 리소스 소모, 비용, 시간 지연 발생. 또한, llm이 결과를 다시 이용하여 결과를 내는 경우, 출력의 부정확성이 발생.

기대효과

1. 요약 품질 향상
- llm이 긍정/부정을 구분해 보면서 전체 요약 생성
- 긍정/부정 각각을 따로 요약하는 것보다 균형 잡힌 요약 가능
- 전체 맥락을 한 번에 파악해 일관성 있는 요약 생성

2. 토큰 절감
- task 3개 -> task 1개

3. 작업 단순화
- 프롬프트 단순화: "총 요약만 수행"
- LLM 작업 부담 감소
- 오류 가능성 감소

4. 성능 향상
- 출력 토큰 감소로 생성 시간 단축
- max_new_tokens 감소 가능(150 -> 100 정도)

---

strength 변경

기존

타겟 음식점 긍정 리뷰 추출 -> llm 요약
각 음식점에서 긍정 리뷰 추출 -> llm 요약
llm 이 요약들 기반으로 타겟 음식점 강점 추출

현재

qwen2.5-7b-instruct-12만 입력 컨텍스트
토큰 수 70,000(120,000의 58%) 이하 (ex) 음식점 4개 토큰 1500개 -> 그대로, 음식점 5개 토큰 2100개 -> 요약)
타겟 음식점 긍정 리뷰 추출 -> 그대로
각 음식점에서 긍정 리뷰 추출 -> 그대로
llm 이 요약들 기반으로 타겟 음식점 강점 추출

적음(총 토큰 수 70,000 이하) 원본 그대로 반환(요약 x)
많음(총 토큰 수 70,000 이상) 2단계(요약 o)

기대 효과
대부분의 경우 원본 직접 비교 (정확도 향상)
극단적인 경우에만 요약 사용

//--strength 추출 변경--//(위 방법 안쓰고 밑에 방법 씀)

1. 타겟 레스토랑 대표 벡터 생성 이 대표 벡터로 경쟁 음식점들(유사 음식점) 찾기
2. 타겟 레스토랑의 장점을 뽑아 리스트화 "맛있고 친절하다" -> ["맛있다", "친절하다"]
3. 경쟁 음식점들도 각각 장점 뽑아 리스트 생성, 이 리스트의 리스트 생성.
4. 경쟁 음식점의 장점 리스트 전체 임베딩 화, 타겟 레스토랑 장점 전체 임베딩화. 코사인 유사도로 비교, 유사하지 않다면(threshold 일정 임계점 이하), 타겟 음식점의 차별된 강점.
5. llm이 타깃이 경쟁 음식점들 대비 두드러지는 강점 top-k + 근거 리뷰 반환.

기대효과

대표 벡터 기반 비교군 선정

검색 횟수: 100회 → 1회 (대표 벡터 검색) + 20회 (리뷰 검색) = 21회
처리 시간: 약 5배 단축
관련성 향상: 유사한 레스토랑만 비교

차별점 계산 개선 (임베딩 기반 Set 비교)

객관성 향상: 임베딩 기반 비교
일관성 향상: LLM 변동성 감소
명확성 향상: 차별점이 명확히 구분됨

전체 통합 (Query 기반 검색과 결합)

Phase 1: 효율적 비교군 선정
Phase 2: 객관적 차별점 계산
Query 기반: 사용자 의도 반영
통합: 최고의 품질과 효율성

image 검색 변경

기존

쿼리 -> 벡터 검색 -> 반환

현재

쿼리 -> 벡터 검색 -> 반환 (명확한 쿼리일 경우 쿼리 재빌딩 불필요)

추상적인 쿼리 -> 의미적으로 풍부하게 변경 -> 벡터 검색 -> 반환 (추상적인 쿼리의 경우 쿼리 재빌딩)

항목	기존	개선 후	효과
비교군 선정	모든 레스토랑 검색 (100회)	대표 벡터 검색 (1회) + Top-20	5배 단축
차별점 계산	LLM 의존 (변동성 높음)	임베딩 기반 Set 비교	객관성 향상
관련성	모든 레스토랑 비교	유사 레스토랑만 비교	관련성 향상
사용자 의도	부분 반영	Query 기반 검색	의도 반영


쿼리 재빌딩

작동 구조 

ex) "분위기 좋다" -> 벡터 검색 -> 반환
-> 의도 반영이 필요한 쿼리 -> 의미적으로 풍부하게 변경 -> 벡터 검색 -> 반환

작동 예시

ex) "데이트하기 좋다" -> "데이트" "로맨틱" "달달한" "커플" -> 벡터 검색 -> 반환

변경 이유

현재 이 기능을 제외한 모든 서비스는 LLM 서비스이다. 이 기능만 LLM을 이용하지 않는다면, AI 서비스의 일관성이 떨어지고 산만한 느낌을 준다고 판단했다.

또한, 사용자가 추상적인 쿼리를 던지더라도, LLM이 쿼리빌더 역할을 해 더 사용자에 의도에 맞는 검색이 되게끔 쿼리를 재빌딩해 내놓는다면, 벡터 Dense 검색 관점에서 사용자의 편의성이 많이 개선되리라고 봤다.

---
---

260113

strength 로직 변경

1. 타겟 음식점에서 리뷰 300개 추출
2. 강점 후보 생성 llm이 리뷰에서 aspect, claim
3. 강점별 근거 확장/검증
    - 이 근거 문장(근거 벡터)(실제 쿼리엔 근거 문장을 간소화하여 검색) 실제 리뷰에서 얼마나 반복되는지(support_count) 검증. support_count < 5면 제거(-> 희소/할루시네이션)
4. 중복 강점 머지
    - sim >= 0.88 -> 무조건 머지(그래프 간선 연결)
    - 0.82 <= sim <= 0.88 -> 근거 리뷰(evidence overlap 통과시(예시 (30%), 30% 겹치면 합치기)
5. 점수화
    - rep_score = log(1 + support_count) * consistency * recency_weight
6. 차별된 강점
    - 비교군 음식점 집합
    - 


1. 타겟 음식점의 장점 추출
2. 이 장점을 llm이 aspect, 근거 문장 등으로 구조화 출력
3. 이 aspect들을 다시 의미가 비슷하다면 하나의 aspect로 병함해 여러 aspect 그룹이 생성.
4. 레스토랑의 모든 리뷰 검색
5. 각 리뷰의 벡터 추출
6. 가중치 계산
- 최근 1년: 1.0
- 1~2년 전: 0.8
- 2년 이상: 0.6
Rating 가중치
- 추천 -> 1.5배
7. 가중 평균으로 대표 벡터 계산


4. 이 aspect들로 aspect 각각으로 비교 음식점(타겟 음식점과 같은 음식점 카테고리인) 각각에 검색 진행
5. 타겟의 각 aspect와 비교 음식점들의 aspect의 문장을 유사도 기반으로 비교, 만약, aspect의 문장의 유사도가 비슷하다면, 그건 타겟 음식점의 강점이 아니고, 유사도가 비슷하지 않다면, 그건 타겟 음식점의 강점임. 그 강점을 input으로 사용해서 llm이 이 음식점의 강점 추출로써 답변을 내놓음. 

260114

summary, strength


서버 전략
1차
Fastapi 서버, VectorDB(Qdrant) 서버(CPU), LLM(GPU)(RUNPOD VLLM SERVERLESS ENDPOINT)

1차의 한계
RUNPOD VLLM SERVERLESS는 vllm에 대한 자유도가 제한적, 하지만, 실제 서비스 환경에선 다양한 상황, 요구사항이 생겨 vllm의 커스터마이징이 필요할 가능성이 있음. 따라서 vllm의 자유도를 높여 다양한 상황, 요구사항에 대응이 가능하게 해야함.

2차
Fastapi 서버, Qdrant(On-disk), watchdog(gpu 서버 종료 및 모니터링용), LLM(GPU 서버(RUNPOD POD))(vllm 서빙)

on-disk
서버를 안띄워도 됨.
대용량 처리에도 적합.
MMAP 기술이 적용됨.

fastapi-watchdog-RUNPOD POD
watchdog이 runpod의 pod을 종료, 모니터링 수행.
이를 통해, 서버리스 환경을 만들 수 있음.



## 기능 외적인 수정,추가


260110

데이터 규모별 통합 테스트 구현
이유:
데이터 규모별 통합 테스트를 통해 현재 기능이 규모별로 잘 돌아가는지 확인을 할 수 있음.

성능 벤치마크 스크립트 작성(지연 시간(latency), 처리량(Throughput),GPU 사용률(상시 GPU서버 사용시), 병목 분석)
이유:
모델 추론 성능의 테스트가 필요했음.

'모델 API 설계' 문서 간략화

'서비스 아키텍쳐 모듈화' 문서 간략화

음식점 별, 음식점 별+음식점 간 배치 비동기 큐 방식 구현(세마포,동적 배치 크기 구현)

---
---

260111

precision@k 도입
이유:
벡터검색의 성능을 평가할 지표가 필요했음

---

디버그용 헤더 추가 (-H "X-Debug: true")
이유:
기존엔 외부사용자용 출력값만 존재.
내부 분석용 정보가 필요했음.
on-off 방식의 장점 -
디버그 모드로 외부 사용자용 출력과 디버그용 출력 명확하게 구분 가능

예시:
# 디버그 모드 (쿼리 파라미터)
curl -X POST "http://localhost:8000/api/v1/sentiment/analyze?debug=true" \
  -H "Content-Type: application/json" \
  -d '{"restaurant_id": 1, "reviews": [...]}'
# 응답: {"restaurant_id": 1, "positive_count": 6, ..., "debug": {...}}

# 디버그 모드 (헤더)
curl -X POST "http://localhost:8000/api/v1/sentiment/analyze" \
  -H "Content-Type: application/json" \
  -H "X-Debug: true" \
  -d '{"restaurant_id": 1, "reviews": [...]}'

로그 전체 데이터를 로그 파일로 저장 구현
로그의 중요한 메트릭은 sqlite에 저장 구현

RDB -> API -> RDB/NoSQL 데이터 파이프라인 및 AI 기능의 파이프라인 상 역할에 대해 명시

2차 과제 전체 초안 작성

1차 과제 '모델 추론 최적화' (실험 아직 안함) 작성

Pod GPU 서버용 도커 이미지 빌드

실제 프로덕션시 생길 수 있는 문제와 해결 방안 문서 작성

---
---

2601113

1. vLLM 메트릭 수집
Prefill 시간과 Decode 시간 분리 측정
TTFT (Time To First Token) 자동 계산
TPS (Tokens Per Second) 계산
TPOT (Time Per Output Token) 계산

2. 데이터베이스 스키마 확장
vllm_metrics 테이블로 세부 메트릭 저장
기존 analysis_metrics와 분리하여 관리

3. Goodput 추적
SLA (TTFT < 2초) 기반 실제 처리량 측정
Throughput vs Goodput 비교 가능

---

prefill 비용+decoding 비용 고려 태스크별 우선순위 큐 도입 제안

현재
prefill 비용은 입력 토큰수로 예상이 가능. 따라서 현재는 prefill 비용에 따른 우선순위 큐로 구현.

실험을 통해 decoding의 정도를 파악하고, prefill+decoding으로 변경 필요.

---

기존

Nvidia-smi와 GPUMonitor가 혼재 (기능은 동일)

Nvidia-smi
-> CLI를 통해 서브 프로세스로 호출하는 방식

GPUMonitor
-> 파이썬 라이브러리로 프로세스 생성 x

현재

GPUMonitor로 통일.

이유

GPUMonitor는 파이썬 라이브러리라, nvidia-smi처럼 CLI를 켜 서브 프로세스를 킬 필요가 없음.

---

기존

1. 일부 의사결정 근거 부족 (Qwen2.5-7B-Instruct 선택 근거 부족)
2. Trade-off 분석 부족 (토큰 임계값(70,000) 결정 근거가 명확하지 않음)
3. 측정값 vs 추정값 구분
4. 초보자 접근성 (용어 설명 부족)
5. 문서 간 참조 (문서 간 상호 참조 부족)
6. 설계 문서 필수 요소 평가 (위험 분석: 약점만 나열, 의존성 다이어그램: 텍스트로만 설명)
7. 실용성 및 실행 가능성 평가

현재 

1. 설계 의사결정 문서화 강화 (Trade-off 분석 포함)
2. 용어집 추가로 초보자 접근성 개선
3. 문서 간 참조 강화
4. 측정값 vs 추정값 명확히 구분

---

260114

기존




