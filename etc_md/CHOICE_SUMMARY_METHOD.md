## 요약 방식 선택 (최종)

**결론: 1번 로직 사용**

- **입력**: 벡터 검색으로 선별된 긍정 리뷰 N개 + 부정 리뷰 N개 (예: 3~10개)
- **처리**:
  - (레스토랑 내부 배치 분할 없이) 긍정/부정 각각 aspect 추출
  - positive/negative aspects 기반으로 LLM이 최종 `overall_summary` 생성
  - **음식점 간**에만 세마포어 기반 비동기 처리 적용
- **선택 이유**:
  - 배치 분할로 인한 aspect/claim 불일치(일관성 저하) 리스크를 피함
  - 토큰/비용 예측이 쉽고, 서비스 레이턴시 관리가 단순함

---

## 두 접근 방식 비교

### 1번 방식: 벡터 검색으로 소량 선별 (예: 긍정 3개, 부정 3개)

**장점:**
- 전체 리뷰를 한 번에 보므로 aspect 추출 일관성 유지
- 배치 분할 없이 단일 LLM 호출로 처리
- 음식점 간 배치 비동기 큐 처리 용이
- 토큰 사용량 예측 가능

**단점:**
- 소량 선별로 인한 정보 손실 가능
- 대표성 부족 가능

**현재 구현:**
- 벡터 검색으로 `limit` 개수만큼 검색 (기본값: 10개)
- vLLM 모드에서는 한 음식점 내에서도 배치 분할을 하고 있음 (문제)

### 2번 방식: 전체 리뷰를 배치 단위로 처리

**장점:**
- 모든 리뷰를 고려하여 정보 손실 최소화
- 대량 리뷰 처리 가능

**단점:**
- 배치 간 aspect 중복 가능성
  - 배치 1: `{"aspect": "불맛", "claim": "숫불향이 좋다"}`
  - 배치 2: `{"aspect": "불맛", "claim": "화력이 강하다"}`
  - → 같은 aspect지만 claim이 달라 중복/불일치 발생
- 배치 간 일관성 저하
  - 각 배치가 전체 맥락을 보지 못해 부분적 추출
  - 최종 합산 시 일관성 저하 가능
- 복잡한 집계 로직 필요
  - 배치별 aspect를 합치고 중복 제거/병합 필요

## 현재 구현의 문제점

`summarize_multiple_restaurants_vllm()` 메서드에서:
1. 한 음식점 내에서 배치 분할을 하고 있음 (1954-1976줄)
2. 하지만 `process_single_batch`는 이전 방식(전체 요약만)을 사용 중 (1997줄)
3. aspect 기반 처리가 적용되지 않음

## 권장 사항

1번 방식을 권장합니다.

이유:
1. 정확도: 전체 리뷰를 한 번에 보면 일관된 aspect 추출 가능
2. 단순성: 배치 분할/집계 로직 불필요
3. 효율성: 음식점 간 비동기 큐 처리만으로 충분

구현 제안:
```python
# 벡터 검색으로 소량 선별 (예: 긍정 10개, 부정 10개)
positive_reviews = vector_search.query_similar_reviews(
    query_text="맛있다 좋다 만족",
    restaurant_id=restaurant_id,
    limit=10,  # 소량 선별
)

negative_reviews = vector_search.query_similar_reviews(
    query_text="맛없다 별로 불만",
    restaurant_id=restaurant_id,
    limit=10,  # 소량 선별
)

# 배치 분할 없이 전체를 한 번에 aspect 추출
positive_aspects = _extract_aspects_from_reviews(positive_reviews)
negative_aspects = _extract_aspects_from_reviews(negative_reviews)
overall_summary = _create_final_summary_from_aspects(positive_aspects, negative_aspects)
```

이렇게 하면:
- 배치 분할 제거 → aspect 일관성 유지
- 음식점 간 비동기 큐 처리만 유지
- 정확도 향상

---

## 운영 가이드

### 1. 벡터 검색 Limit 값 선택 가이드

#### 기본값: 10개 (권장)
- **장점**: 대표성 확보, 정보 손실 최소화
- **단점**: 토큰 사용량 증가 (약 2~3배)
- **적용 시나리오**: 
  - 일반적인 프로덕션 환경
  - 정확도가 중요한 경우
  - 리뷰가 다양하고 많은 경우

#### 최소값: 3개
- **장점**: 토큰 사용량 최소화, 빠른 응답
- **단점**: 정보 손실 가능, 대표성 부족
- **적용 시나리오**:
  - 트래픽이 매우 높은 피크 시간
  - 비용 최적화가 중요한 경우
  - 리뷰가 단순하고 유사한 경우

#### 권장 설정 범위: 5~15개
- **5개**: 비용 최적화 + 최소한의 대표성
- **10개**: 기본값 (정확도와 비용의 균형)
- **15개**: 높은 정확도가 필요한 경우

### 2. 토큰 사용량 예측

#### Aspect 추출 단계
- **입력 토큰**: 리뷰 텍스트 (평균 50~100 토큰/리뷰)
  - limit=3: 약 150~300 토큰 (긍정) + 150~300 토큰 (부정) = **300~600 토큰**
  - limit=10: 약 500~1000 토큰 (긍정) + 500~1000 토큰 (부정) = **1000~2000 토큰**
- **출력 토큰**: aspect 리스트 (평균 200~500 토큰)
  - 긍정 aspect 추출: **200~500 토큰**
  - 부정 aspect 추출: **200~500 토큰**

#### Overall Summary 생성 단계
- **입력 토큰**: aspect 정보 (약 100~300 토큰)
- **출력 토큰**: 요약 텍스트 (평균 50~200 토큰)

#### 총 토큰 사용량 (예상)
- **limit=3**: 약 **1,000~2,000 토큰/레스토랑**
- **limit=10**: 약 **2,500~4,000 토큰/레스토랑**

### 3. 트래픽/성능 고려사항

#### 동시 처리 수 제한
- **설정**: `VLLM_MAX_CONCURRENT_BATCHES` (기본값 확인 필요)
- **권장값**: 
  - GPU 메모리 24GB: 4~8개
  - GPU 메모리 40GB: 8~16개
  - GPU 메모리 80GB: 16~32개

#### 응답 시간 예상
- **단일 레스토랑 처리 시간**:
  - limit=3: 약 2~5초
  - limit=10: 약 5~10초
- **배치 처리 (10개 레스토랑)**:
  - limit=3: 약 5~15초 (병렬 처리)
  - limit=10: 약 15~30초 (병렬 처리)

### 4. 모니터링 포인트

#### 필수 메트릭
1. **토큰 사용량**
   - Aspect 추출 단계별 토큰 사용량
   - Overall summary 생성 토큰 사용량
   - 레스토랑당 평균 토큰 사용량

2. **응답 시간**
   - 단일 레스토랑 처리 시간
   - 배치 처리 총 시간
   - P50, P95, P99 레이턴시

3. **처리량 (Throughput)**
   - 초당 처리 레스토랑 수
   - 동시 처리 수 (세마포어 사용률)

4. **에러율**
   - Aspect 추출 실패율
   - JSON 파싱 실패율
   - 전체 요약 실패율

5. **리소스 사용률**
   - GPU 사용률
   - GPU 메모리 사용률
   - CPU 사용률

#### 알림 임계값 (권장)
- **응답 시간 P95 > 30초**: 성능 저하
- **에러율 > 5%**: 시스템 문제
- **GPU 메모리 사용률 > 90%**: OOM 위험
- **동시 처리 수가 항상 최대값**: 병목 발생

### 5. 튜닝 가이드

#### 비용 최적화가 필요한 경우
```python
# API 요청 시
{
    "restaurant_id": 123,
    "positive_query": "맛있다 좋다 만족",
    "negative_query": "맛없다 별로 불만",
    "limit": 3,  # 기본값 10 → 3으로 감소
    "min_score": 0.3  # 유사도 임계값 상향 (더 관련성 높은 리뷰만)
}
```
- **효과**: 토큰 사용량 약 50~60% 감소
- **트레이드오프**: 정보 손실 가능성 증가

#### 정확도 향상이 필요한 경우
```python
{
    "restaurant_id": 123,
    "positive_query": "맛있다 좋다 만족",
    "negative_query": "맛없다 별로 불만",
    "limit": 15,  # 기본값 10 → 15로 증가
    "min_score": 0.0  # 유사도 임계값 하향 (더 많은 리뷰 포함)
}
```
- **효과**: 대표성 및 정확도 향상
- **트레이드오프**: 토큰 사용량 및 응답 시간 증가

#### 처리량 향상이 필요한 경우
```python
# Config 설정
VLLM_MAX_CONCURRENT_BATCHES = 16  # 기본값에서 증가
```
- **효과**: 동시 처리 레스토랑 수 증가
- **트레이드오프**: GPU 메모리 사용량 증가, OOM 위험

### 6. 실제 운영 시나리오별 권장 설정

#### 시나리오 A: 일반 프로덕션 환경
```python
limit = 10  # 기본값
min_score = 0.0
VLLM_MAX_CONCURRENT_BATCHES = 8
```
- **목표**: 정확도와 비용의 균형
- **예상 성능**: 레스토랑당 5~10초, 토큰 2,500~4,000

#### 시나리오 B: 피크 시간 (트래픽 높음)
```python
limit = 5  # 감소
min_score = 0.2  # 상향
VLLM_MAX_CONCURRENT_BATCHES = 12  # 증가
```
- **목표**: 처리량 최대화, 비용 최소화
- **예상 성능**: 레스토랑당 3~6초, 토큰 1,500~2,500

#### 시나리오 C: 정확도 우선 (프리미엄 서비스)
```python
limit = 15  # 증가
min_score = 0.0
VLLM_MAX_CONCURRENT_BATCHES = 4  # 감소 (안정성)
```
- **목표**: 최고 정확도
- **예상 성능**: 레스토랑당 10~20초, 토큰 4,000~6,000

### 7. 주의사항

#### Aspect 추출 토큰 제한
- **현재 설정**: `max_tokens=4000` (기본값)
- **limit=10일 때**: 대부분의 경우 충분
- **limit=15 이상**: 일부 긴 리뷰는 샘플링으로 제외될 수 있음
- **권장**: limit 값에 따라 동적 조정
  ```python
  aspect_max_tokens = min(4000, limit * 200)  # 리뷰당 평균 200 토큰 가정
  ```

#### 벡터 검색 품질
- **min_score 임계값**: 너무 높으면 관련 리뷰 누락 가능
- **쿼리 텍스트**: 기본값("맛있다 좋다 만족")이 모든 레스토랑에 적합하지 않을 수 있음
- **권장**: 레스토랑 카테고리별 맞춤 쿼리 사용

#### 에러 처리
- **Aspect 추출 실패**: 빈 리스트 반환 → overall_summary는 "요약할 내용이 없습니다" 반환
- **JSON 파싱 실패**: 재시도 로직 없음 → 에러 로그 기록 후 빈 결과 반환
- **권장**: 재시도 로직 추가 고려

### 8. 비용 예측 (참고)

#### LLM API 비용 (예시, 실제 비용은 제공업체 확인 필요)
- **입력 토큰**: $0.0001 / 1K 토큰
- **출력 토큰**: $0.0003 / 1K 토큰

#### 레스토랑당 예상 비용
- **limit=3**: 약 $0.0003~0.0006
- **limit=10**: 약 $0.0008~0.0012

#### 월간 비용 예측 (10,000개 레스토랑 처리)
- **limit=3**: 약 $3~6
- **limit=10**: 약 $8~12

---

## 변경 이력

- **2024-XX-XX**: 1번 로직 적용 완료 (레스토랑 내부 배치 분할 제거)
- **2024-XX-XX**: 운영 가이드 추가